#!/usr/bin/env python3
"""
Feedly æ–‡ç« è¿‡æ»¤å™¨

ç»Ÿä¸€çš„æ–‡ç« è¿‡æ»¤å·¥å…·ï¼Œæ”¯æŒå¤šç§è¿‡æ»¤æ¨¡å¼ï¼š
- newsflash: è¿‡æ»¤ 36kr å¿«è®¯
- low-score: è¿‡æ»¤ä½åˆ†æ–‡ç« 
- all: ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰è¿‡æ»¤

ä½¿ç”¨æ–¹æ³•ï¼š
    python feedly_filter.py newsflash [--limit 500] [--dry-run]
    python feedly_filter.py low-score [--limit 100] [--threshold 2.5] [--dry-run]
    python feedly_filter.py all [--limit 200] [--threshold 2.5] [--dry-run]
"""

import argparse
import logging
import sys
from dataclasses import dataclass
from typing import Callable

from rss_analyzer.config import PROJ_CONFIG, setup_logging
from rss_analyzer.feedly_client import feedly_fetch_unread, feedly_mark_read
from rss_analyzer.article_fetcher import fetch_article_content
from rss_analyzer.llm_analyzer import analyze_article_with_llm, analyze_articles_with_llm_batch
from rss_analyzer.utils import is_newsflash
from rss_analyzer.cache import get_cached_score, save_cached_score

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class FilterResult:
    """è¿‡æ»¤ç»“æœ"""
    matched: list      # åŒ¹é…çš„æ–‡ç« ï¼ˆå°†è¢«æ ‡è®°ä¸ºå·²è¯»ï¼‰
    remaining: list    # å‰©ä½™æ–‡ç« ï¼ˆæœªåŒ¹é…ï¼‰
    label: str         # æ ‡ç­¾ï¼ˆç”¨äºæ—¥å¿—ï¼‰


# 36kr Feed æº ID
FEED_ID_36KR = "feed/http://www.36kr.com/feed"


# ============================================================================
# Core Functions
# ============================================================================

def fetch_articles(limit: int, stream_id: str = None) -> list:
    """è·å– Feedly æœªè¯»æ–‡ç« """
    source_label = "36kræº" if stream_id == FEED_ID_36KR else "æ‰€æœ‰æœªè¯»"
    logger.info(f"ğŸ“¥ ä» [{source_label}] è·å–æœªè¯»æ–‡ç«  (limit={limit})...")
    
    articles = feedly_fetch_unread(stream_id=stream_id, limit=limit) or []
    logger.info(f"âœ… è·å–åˆ° {len(articles)} ç¯‡")
    return articles


def mark_as_read(articles: list, label: str, dry_run: bool) -> bool:
    """æ ‡è®°æ–‡ç« ä¸ºå·²è¯»"""
    if not articles:
        return True
    
    ids = [a['id'] for a in articles if a.get('id')]
    
    if dry_run:
        logger.info(f"[DRY RUN] å°†æ ‡è®° {len(ids)} ç¯‡{label}æ–‡ç« :")
        for a in articles[:5]:
            score = a.get('_score')
            prefix = f"[{score:.1f}] " if score else ""
            logger.info(f"  - {prefix}{a.get('title', '')[:50]}")
        if len(articles) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(articles) - 5} ç¯‡")
        return True
    
    for i in range(0, len(ids), 500):
        if not feedly_mark_read(ids[i:i+500]):
            logger.error(f"æ ‡è®°å¤±è´¥: {i+1}-{i+500}")
            return False
    
    logger.info(f"âœ… å·²æ ‡è®° {len(ids)} ç¯‡{label}æ–‡ç« ")
    return True


def run_filters(articles: list, filters: list[Callable], dry_run: bool) -> int:
    """ä¾æ¬¡è¿è¡Œå¤šä¸ªè¿‡æ»¤å™¨"""
    remaining = articles
    total_matched = 0
    
    for filter_func in filters:
        if not remaining:
            break
        result = filter_func(remaining)
        mark_as_read(result.matched, result.label, dry_run)
        total_matched += len(result.matched)
        remaining = result.remaining
    
    logger.info(f"ğŸ“Š æ€»è®¡è¿‡æ»¤: {total_matched}/{len(articles)}")
    return 0


# ============================================================================
# Filters
# ============================================================================

def newsflash_filter(articles: list) -> FilterResult:
    """å¿«è®¯è¿‡æ»¤å™¨"""
    # æ—¢ç„¶å·²ç»æŒ‡å®šäº†æºï¼Œå¯èƒ½å¤§éƒ¨åˆ†éƒ½æ˜¯å¿«è®¯ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿˜æ˜¯ä¿ç•™ is_newsflash æ£€æŸ¥
    # æˆ–è€…å¦‚æœ 36kr æºé‡ŒåŒ…å«éå¿«è®¯çš„æ™®é€šæ–‡ç« ï¼Œè¿™ä¸ªæ£€æŸ¥å°±æ˜¯å¿…è¦çš„
    matched = [a for a in articles if is_newsflash(a)]
    remaining = [a for a in articles if not is_newsflash(a)]
    logger.info(f"ğŸ—ï¸ å¿«è®¯: {len(matched)}/{len(articles)}")
    return FilterResult(matched, remaining, "å¿«è®¯")


def low_score_filter(articles: list, threshold: float = 3.0, dry_run: bool = False) -> FilterResult:
    """ä½åˆ†æ–‡ç« è¿‡æ»¤å™¨ï¼Œè°ƒç”¨ LLM å¯¹æ–‡ç« è¿›è¡Œè¯„åˆ†å¹¶æ ¹æ®é˜ˆå€¼è¿‡æ»¤"""
    matched, remaining = [], []
    batch_scoring = PROJ_CONFIG.get("batch_scoring", False)
    batch_size = max(1, int(PROJ_CONFIG.get("batch_size", 1)))
    batch_queue = []

    for i, article in enumerate(articles, 1):
        title = article.get('title', '')[:50]
        prefix = f"[{i}/{len(articles)}]"
        article_id = article.get('id')

        # 1. Check Cache
        cached = get_cached_score(article_id)
        if cached:
            score = cached['score']
            logger.info(f"{prefix} â™»ï¸ ä½¿ç”¨ç¼“å­˜è¯„åˆ†: {title}")
            _handle_scored_article(
                article,
                score,
                prefix,
                threshold,
                dry_run,
                matched,
                remaining
            )
            continue

        logger.info(f"{prefix} è¯„åˆ†ä¸­: {title}...")

        if batch_scoring:
            batch_queue.append({
                "article": article,
                "prefix": prefix,
                "payload": _prepare_article_scoring(article)
            })
            if len(batch_queue) >= batch_size:
                batch_payload = [item["payload"] for item in batch_queue]
                batch_results = analyze_articles_with_llm_batch(batch_payload)
                for item, analysis in zip(batch_queue, batch_results):
                    score = analysis.get("score", 0.0)
                    # Save to Cache
                    save_cached_score(item["article"].get('id'), score, analysis)

                    _handle_scored_article(
                        item["article"],
                        score,
                        item["prefix"],
                        threshold,
                        dry_run,
                        matched,
                        remaining
                    )
                batch_queue = []
        else:
            score, analysis = _score_article(article)
            # Save to Cache (if valid)
            if score >= 0:
                save_cached_score(article_id, score, analysis)

            _handle_scored_article(
                article,
                score,
                prefix,
                threshold,
                dry_run,
                matched,
                remaining
            )

    if batch_scoring and batch_queue:
        batch_payload = [item["payload"] for item in batch_queue]
        batch_results = analyze_articles_with_llm_batch(batch_payload)
        for item, analysis in zip(batch_queue, batch_results):
            score = analysis.get("score", 0.0)
            # Save to Cache
            save_cached_score(item["article"].get('id'), score, analysis)

            _handle_scored_article(
                item["article"],
                score,
                item["prefix"],
                threshold,
                dry_run,
                matched,
                remaining
            )
        batch_queue = []

    logger.info(f"ğŸ“Š è¿‡æ»¤ç»“æœ: {len(matched)} ç¯‡è¿‡æ»¤, {len(remaining)} ç¯‡ä¿ç•™")
    return FilterResult(matched, remaining, "ä½åˆ†")


def _score_article(article: dict) -> tuple[float, dict]:
    """å¯¹æ–‡ç« è¿›è¡Œè¯„åˆ†ï¼Œè¿”å› (score, analysis_data)"""
    payload = _prepare_article_scoring(article)

    # è°ƒç”¨é…ç½®çš„åˆ†ææ¨¡å‹è¿›è¡Œè¯„åˆ†
    try:
        result = analyze_article_with_llm(
            payload.get("title", ""),
            payload.get("summary", ""),
            payload.get("content", "")
        )
        return result.get('score', 0.0), result
    except Exception as e:
        logger.debug(f"è¯„åˆ†å‡ºé”™: {e}")
        return -1.0, {}


def _prepare_article_scoring(article: dict) -> dict:
    """å‡†å¤‡æ–‡ç« è¯„åˆ†æ‰€éœ€çš„ Payload"""
    title, summary = article.get('title', ''), article.get('summary', '')
    content = article.get('content', '')

    if not (content and len(content) > 200):
        content = summary if len(summary) > 500 else _fetch_content(article) or summary

    return {
        "title": title,
        "summary": summary,
        "content": content
    }


def _handle_scored_article(article: dict, score: float, prefix: str, threshold: float, dry_run: bool,
                           matched: list, remaining: list) -> None:
    """å¤„ç†è¯„åˆ†åçš„æ–‡ç« ï¼Œå†³å®šæ˜¯æ ‡è®°å·²è¯»è¿˜æ˜¯ä¿ç•™"""
    
    title_str = article.get('title', 'Unknown Title')
    if score < 0:
        logger.info(f"{prefix} ç»“æœ: è·³è¿‡ (è¯„åˆ†å¤±è´¥)")
        remaining.append(article)
    elif score <= threshold:
        article_id = article.get('id')
        if article_id and not dry_run:
            feedly_mark_read([article_id])
            logger.info(f"{prefix} ç»“æœ: âŒæ ‡é¢˜: {title_str}")
            logger.info(f"{prefix} ç»“æœ: {score:.1f} åˆ† (ä½äºé˜ˆå€¼ï¼Œå·²æ ‡è®°å·²è¯»)")
        else:
            logger.info(f"{prefix} ç»“æœ: âŒæ ‡é¢˜: {title_str}")
            logger.info(f"{prefix} ç»“æœ: {score:.1f} åˆ† (ä½äºé˜ˆå€¼ï¼Œ[DRY RUN] è·³è¿‡æ ‡è®°)")
        matched.append({**article, '_score': score})
    else:
        logger.info(f"{prefix} ç»“æœ: âœ…æ ‡é¢˜: {title_str}")
        logger.info(f"{prefix} ç»“æœ: {score:.1f} åˆ† (ä¿ç•™)")
        remaining.append(article)


def _fetch_content(article: dict) -> str:
    """æŠ“å–æ–‡ç« å†…å®¹"""
    link = article.get('canonicalUrl') or article.get('alternate', [{}])[0].get('href', '')
    return fetch_article_content(link) if link else ""


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Feedly æ–‡ç« è¿‡æ»¤å™¨')
    parser.add_argument('--debug', action='store_true')
    parser.add_argument('--limit', '-l', type=int, default=500, help='è·å–æ–‡ç« æ•°é‡')
    parser.add_argument('--threshold', '-t', type=float, default=3.0, help='ä½åˆ†é˜ˆå€¼')
    parser.add_argument('--dry-run', '-n', action='store_true', help='æ¨¡æ‹Ÿæ¨¡å¼')
    
    sub = parser.add_subparsers(dest='cmd')
    sub.add_parser('newsflash', help='è¿‡æ»¤å¿«è®¯')
    sub.add_parser('low-score', help='è¿‡æ»¤ä½åˆ†')
    sub.add_parser('all', help='å…¨é‡è¿‡æ»¤')
    
    args = parser.parse_args()
    
    if args.debug:
        setup_logging(True)
    
    # é»˜è®¤ä½¿ç”¨ all å‘½ä»¤
    if not args.cmd:
        args.cmd = 'all'
    
    # ç­–ç•¥è·¯ç”±
    if args.cmd == 'newsflash':
        # ä¸“é—¨ä» 36kr æºè·å–
        articles = fetch_articles(args.limit, stream_id=FEED_ID_36KR)
        filters = [newsflash_filter]
    elif args.cmd == 'low-score':
        # ä»å…¨å±€è·å–
        articles = fetch_articles(args.limit)
        filters = [lambda a: low_score_filter(a, args.threshold, args.dry_run)]
    else:  # all
        # å…¨é‡æ¨¡å¼é€»è¾‘ï¼š
        # 1. å…ˆè·‘ä¸€éå¿«è®¯è¿‡æ»¤ï¼ˆé’ˆå¯¹æ€§æ¸…ç†ï¼‰- å¯é€‰ï¼Œæˆ–è€…ç›´æ¥ç”±å…¨å±€å¤„ç†è¦†ç›–
        # 2. å†è·‘å…¨å±€
        # ä¸ºäº†ç®€å•ä¸”ç¬¦åˆ"all"çš„è¯­ä¹‰ï¼ˆå¤„ç†æ‰€æœ‰æœªè¯»ï¼‰ï¼Œè¿™é‡Œæˆ‘ä»¬åªåšä¸€æ¬¡å…¨å±€ fetch
        # å¦‚æœç”¨æˆ·å¸Œæœ›åˆ†å¼€è·‘ï¼Œåº”è¯¥åˆ†åˆ«è°ƒç”¨ newsflash å’Œ low-score
        
        # ä¿®æ­£ï¼šæ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œå¯èƒ½å¸Œæœ› all ä¹Ÿèƒ½äº«å—åˆ°é’ˆå¯¹æ€§è¿‡æ»¤çš„å¥½å¤„ï¼Ÿ
        # ä½†"all"é€šå¸¸æ„å‘³ç€å¤„ç†æ‰€æœ‰æ¥æºã€‚å¦‚æœåª fetch 36krï¼Œå°±æ¼äº†åˆ«çš„ã€‚
        # å¦‚æœ fetch globalï¼Œä¹ŸåŒ…å« 36krã€‚
        # æ‰€ä»¥ all æ¨¡å¼ç»´æŒåŸæ ·ï¼ˆfetch globalï¼‰ï¼Œä½†åº”ç”¨æ‰€æœ‰è¿‡æ»¤å™¨ã€‚
        
        articles = fetch_articles(args.limit)
        filters = [newsflash_filter, lambda a: low_score_filter(a, args.threshold, args.dry_run)]

    if not articles:
        return 0
    
    return run_filters(articles, filters, args.dry_run)


if __name__ == '__main__':
    sys.exit(main())
