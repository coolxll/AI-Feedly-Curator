"""
æ–‡ç« è¯„åˆ†æ¨¡å—
åŸºäºå¤šç»´åº¦è¯„ä¼°æ–‡ç« çš„é˜…è¯»ä»·å€¼
"""
import json
import re
import logging
from typing import Dict, Any
from datetime import datetime

from openai import OpenAI

from .config import PROJ_CONFIG, get_config, log_debug

logger = logging.getLogger(__name__)


# è¯„åˆ†ç»´åº¦é…ç½® (ä¿ç•™ Metadataï¼Œæƒé‡ç°åœ¨æ˜¯åŠ¨æ€çš„)
SCORING_DIMENSIONS = {
    "relevance": {"name": "ç›¸å…³æ€§"},
    "informativeness_accuracy": {"name": "ä¿¡æ¯é‡ä¸å‡†ç¡®æ€§"},
    "depth_opinion": {"name": "æ·±åº¦ä¸è§‚ç‚¹"},
    "readability": {"name": "å¯è¯»æ€§"},
    "non_redundancy": {"name": "åŸåˆ›æ€§/æ°´åˆ†åº¦"}
}

# åŠ¨æ€æƒé‡é…ç½®
DEFAULT_WEIGHTS = PROJ_CONFIG.get("scoring_weights", {}).get("default", {
    "relevance": 2, "informativeness_accuracy": 2, "depth_opinion": 2, "readability": 2, "non_redundancy": 1
})

# è´Ÿé¢æ¸…å•é…ç½®
RED_FLAGS = [
    "pure_promotion",  # çº¯æ¨å¹¿/è½¯æ–‡ (Soft)
    "clickbait",       # æ ‡é¢˜å…š (Soft)
    "ai_generated"     # AI ç”Ÿæˆæ„Ÿè¿‡é‡/é€»è¾‘æ··ä¹± (Hard)
]

HARD_RED_FLAGS = {"ai_generated"}


def _build_content_snippet(content: str) -> str:
    """æ™ºèƒ½æˆªæ–­æ­£æ–‡ï¼Œä¿ç•™å¤´å°¾å…³é”®ä¿¡æ¯ã€‚"""
    if len(content) > 10000:
        return content[:6000] + "\n\n...[å†…å®¹è¿‡é•¿ï¼Œä¸­é—´éƒ¨åˆ†çœç•¥]...\n\n" + content[-3000:]
    return content


def build_scoring_prompt(title: str, summary: str, content: str) -> str:
    """æ„å»ºç»“æ„åŒ–è¯„åˆ†æç¤ºè¯ (å«æ€ç»´é“¾ & æ™ºèƒ½æˆªæ–­)"""
    persona = PROJ_CONFIG.get("scoring_persona", "")
    
    # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å¤´éƒ¨å’Œå°¾éƒ¨ï¼Œä¸­é—´æˆªæ–­
    content_snippet = _build_content_snippet(content)

    today_str = datetime.now().strftime("%Y-%m-%d")

    return f"""{persona}

å½“å‰æ—¥æœŸï¼š{today_str}

è¯·æ ¹æ®ä½ çš„ä¸“ä¸šèƒŒæ™¯ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–‡ç« è¿›è¡Œæ·±åº¦è¯„ä¼°ï¼š

### ç¬¬ä¸€æ­¥ï¼šåˆ†æä¸æ€è€ƒ (Chain of Thought)
è¯·å…ˆé€šè¯»å…¨æ–‡ï¼Œ**é¦–å…ˆåˆ¤æ–­æ–‡ç« ç±»å‹**ï¼Œç„¶åæ ¹æ®ç±»å‹ç‰¹ç‚¹è¿›è¡Œè¯„ä¼°ï¼š

**ç±»å‹è¯†åˆ«æŒ‡å—**ï¼š
- **newsï¼ˆèµ„è®¯ï¼‰**: æŠ¥é“äº‹ä»¶ã€æ•°æ®ã€å‘å¸ƒæ¶ˆæ¯ï¼Œé‡ç‚¹æ˜¯â€œæ˜¯ä»€ä¹ˆâ€
- **tutorialï¼ˆæ•™ç¨‹ï¼‰**: æ•™æˆæŠ€èƒ½ã€å±•ç¤ºæ­¥éª¤ã€æä¾›ä»£ç ï¼Œé‡ç‚¹æ˜¯â€œæ€ä¹ˆåšâ€
- **opinionï¼ˆè§‚ç‚¹ï¼‰**: åˆ†æé—®é¢˜ã€è¯„è®ºç°è±¡ã€æå‡ºè§è§£ï¼Œé‡ç‚¹æ˜¯â€œä¸ºä»€ä¹ˆâ€

**é’ˆå¯¹æ€§åˆ†æ**ï¼š
- èµ„è®¯ç±»ï¼šå…³æ³¨æ—¶æ•ˆæ€§ã€å‡†ç¡®æ€§ã€æ•°æ®å®Œæ•´æ€§
- æ•™ç¨‹ç±»ï¼šå…³æ³¨å¯å¤ç°æ€§ã€æ­¥éª¤æ¸…æ™°åº¦ã€å®é™…è§£å†³é—®é¢˜
- è§‚ç‚¹ç±»ï¼šå…³æ³¨è®ºè¯æ·±åº¦ã€ç‹¬åˆ°è§è§£ã€é€»è¾‘è‡ªæ´½

é€šç”¨æ£€æŸ¥ï¼š
- æ˜¯å¦æœ‰è¿‡åº¦çš„è¥é”€è¯æœ¯æˆ–è¯¯å¯¼æ€§æ ‡é¢˜ï¼Ÿ
- æ˜¯å¦æœ‰æ˜æ˜¾çš„AIç”Ÿæˆç—•è¿¹æˆ–é€»è¾‘æ··ä¹±ï¼Ÿ

### ç¬¬äºŒæ­¥ï¼šç±»å‹åˆ¤æ–­ & è´Ÿé¢æ£€æµ‹
1. **åˆ¤æ–­æ–‡ç« ç±»å‹**ï¼š`news` (èµ„è®¯), `tutorial` (æ•™ç¨‹), `opinion` (è§‚ç‚¹).
2. **æ£€æµ‹è´Ÿé¢ç‰¹å¾**ï¼š
   - `pure_promotion`: çº¯å¹¿å‘Š/å–è¯¾/æ¨é”€äº§å“ (Soft Flag). **æ³¨æ„ï¼šæŠ€æœ¯è§†è§’çš„å·¥å…·æ¨èã€æ–°åŠŸèƒ½å‘å¸ƒã€å¼€æºé¡¹ç›®ä»‹ç»ã€æŠ•è¡ŒæŠ¥å‘Šæ‘˜è¦å‡ä¸ç®—å¹¿å‘Šã€‚**
   - `clickbait`: æ ‡é¢˜å…š (Soft Flag)
   - `ai_generated`: æ˜æ˜¾çš„ AI ç”Ÿæˆç—•è¿¹/é€»è¾‘æ··ä¹± (Hard Flag)

### ç¬¬ä¸‰æ­¥ï¼šå¤šç»´åº¦æ‰“åˆ† (1-5åˆ†ï¼Œæ ¹æ®ç±»å‹çµæ´»è¯„åˆ†)

> **é‡è¦ï¼šä¸åŒç±»å‹æ–‡ç« çš„è¯„åˆ†ä¾§é‡ç‚¹ä¸åŒï¼**

#### é€šç”¨è¯„åˆ†æ ‡å‡†
- 5åˆ†ï¼šæä½³
- 4åˆ†ï¼šä¼˜ç§€
- 3åˆ†ï¼šåŠæ ¼ï¼ˆå¤§éƒ¨åˆ†æ–‡ç« åœ¨æ­¤åŒºé—´ï¼‰
- 1-2åˆ†ï¼šå·®

#### ç»´åº¦è¯¦è§£ï¼ˆè¯·æ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´è¯„åˆ†é‡ç‚¹ï¼‰

1. **ç›¸å…³æ€§**ï¼ˆæ‰€æœ‰ç±»å‹çš„é¦–è¦ç»´åº¦ï¼‰
   - æ˜¯å¦ç¬¦åˆç”¨æˆ·äººè®¾ï¼ˆTech / æŠ•èµ„ / å›½é™…æ”¿æ²»ï¼‰
   - âš ï¸ å¦‚æœç›¸å…³æ€§ < 2.5ï¼Œå³ä½¿å…¶ä»–ç»´åº¦å†é«˜ï¼Œæ–‡ç« ä¹Ÿä¸æ¨è

2. **ä¿¡æ¯é‡ä¸å‡†ç¡®æ€§**
   - **news**: æ ¸å¿ƒç»´åº¦ï¼æ•°æ®æ˜¯å¦å®Œæ•´ã€æ¥æºæ˜¯å¦å¯é ã€æ˜¯å¦æœ‰æ–°å¢ä¿¡æ¯
   - **tutorial**: æ­¥éª¤æ˜¯å¦è¯¦å°½ã€ä»£ç æ˜¯å¦å¯ç”¨ã€æ˜¯å¦è§£å†³çœŸå®é—®é¢˜
   - **opinion**: è®ºæ®æ˜¯å¦å……åˆ†ã€äº‹å®æ˜¯å¦å‡†ç¡®

3. **æ·±åº¦ä¸è§‚ç‚¹**ï¼ˆæƒé‡å› ç±»å‹è€Œå¼‚ï¼‰
   - **news**: âš ï¸ ä¸å¼ºæ±‚æ·±åº¦åˆ†æï¼å¦‚æœæ˜¯åŠæ—¶ã€å‡†ç¡®çš„å¸‚åœºåŠ¨æ€/è¡Œä¸šèµ„è®¯ï¼Œå³ä½¿åªæ˜¯æ•°æ®æ±‡æ€»ï¼Œä¹Ÿåº”ç»™ â‰¥3åˆ†
   - **tutorial**: æ˜¯å¦åŸºäºå®è·µç»éªŒã€æ˜¯å¦è¦†ç›–å¸¸è§å‘ç‚¹
   - **opinion**: æ ¸å¿ƒç»´åº¦ï¼æ˜¯å¦æœ‰ç‹¬åˆ°è§è§£ã€è®ºè¯æ˜¯å¦æ·±åˆ»

4. **å¯è¯»æ€§**
   - ç»“æ„æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦é¡ºç•…
   - tutorialç±»ï¼šæ­¥éª¤ç¼–å·ã€ä»£ç æ ¼å¼æ˜¯å¦æ¸…æ™°ï¼ˆæƒé‡æ›´é«˜ï¼‰

5. **åŸåˆ›æ€§/æ°´åˆ†åº¦**
   - news: æ˜¯å¦é¦–å‘ã€æ˜¯å¦æ´—ç¨¿
   - tutorial: âš ï¸ å¯¹äºtroubleshootingï¼ˆæ’é”™ï¼‰ç±»æ–‡ç« ï¼Œå³ä½¿æ–¹æ¡ˆå¸¸è§ä½†åªè¦è§£å†³äº†å…·ä½“é—®é¢˜ï¼Œä¹Ÿåº”ç»™ â‰¥3åˆ†
   - opinion: æ˜¯å¦æ‹„è¢–ã€æ˜¯å¦æ³›æ³›è€Œè°ˆ

### è¾“å‡ºæ ¼å¼ (JSON Only)
è¯·åªè¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ï¼š
{{
  "analysis": "1-2å¥è¯çš„ç®€è¦åˆ†æï¼Œè¯´æ˜æ‰“åˆ†ç†ç”±",
  "article_type": "news/tutorial/opinion",
  "red_flags": ["clickbait", ...],  // æ— åˆ™ä¸ºç©ºæ•°ç»„
  "scores": {{
    "relevance": <1-5>,
    "informativeness_accuracy": <1-5>,
    "depth_opinion": <1-5>,
    "readability": <1-5>,
    "non_redundancy": <1-5>
  }}
}}

---
æ–‡ç« ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary[:200] if summary else 'æ— '}
æ­£æ–‡ï¼š
{content_snippet}
"""


def build_batch_scoring_prompt(articles: list[dict]) -> str:
    """æ„å»ºæ‰¹é‡è¯„åˆ†æç¤ºè¯ (JSON æ•°ç»„è¾“å‡º)"""
    persona = PROJ_CONFIG.get("scoring_persona", "")
    today_str = datetime.now().strftime("%Y-%m-%d")

    items = []
    for idx, article in enumerate(articles):
        content_snippet = _build_content_snippet(article.get("content", ""))
        items.append(
            f"""
### æ–‡ç«  {idx}
æ ‡é¢˜ï¼š{article.get("title", "")}
æ‘˜è¦ï¼š{(article.get("summary") or "")[:200] if article.get("summary") else "æ— "}
æ­£æ–‡ï¼š
{content_snippet}
""".strip()
        )

    articles_block = "\n\n".join(items)

    return f"""{persona}

å½“å‰æ—¥æœŸï¼š{today_str}

è¯·æ ¹æ®ä½ çš„ä¸“ä¸šèƒŒæ™¯ï¼Œå¯¹ä¸‹é¢å¤šç¯‡æ–‡ç« é€ä¸€è¯„åˆ†ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š
- è¾“å‡ºå¿…é¡»æ˜¯ JSON æ•°ç»„ï¼Œæ•°ç»„å†…æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ç¯‡æ–‡ç« 
- æ¯ä¸ªå…ƒç´ å¿…é¡»åŒ…å« "index" å­—æ®µï¼ˆä¸æ–‡ç« ç¼–å·ä¸€è‡´ï¼‰
- ä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬

è¯„åˆ†æµç¨‹ä¸è¦æ±‚ï¼š
1. å…ˆåˆ¤æ–­æ–‡ç« ç±»å‹ï¼š`news`ï¼ˆèµ„è®¯ï¼‰, `tutorial`ï¼ˆæ•™ç¨‹ï¼‰, `opinion`ï¼ˆè§‚ç‚¹ï¼‰
2. æ£€æµ‹è´Ÿé¢ç‰¹å¾ï¼š`pure_promotion`, `clickbait`, `ai_generated`
3. æŒ‰ç»´åº¦æ‰“åˆ†ï¼ˆ1-5ï¼‰ï¼šrelevance, informativeness_accuracy, depth_opinion, readability, non_redundancy

è¾“å‡ºæ ¼å¼ï¼ˆJSON Onlyï¼‰ï¼š
[
  {{
    "index": 0,
    "analysis": "1-2å¥è¯çš„ç®€è¦åˆ†æ",
    "article_type": "news/tutorial/opinion",
    "red_flags": ["clickbait"],
    "scores": {{
      "relevance": 1-5,
      "informativeness_accuracy": 1-5,
      "depth_opinion": 1-5,
      "readability": 1-5,
      "non_redundancy": 1-5
    }}
  }}
]

---
{articles_block}
"""


def calculate_weighted_score(scores: Dict[str, int], article_type: str, red_flags: list) -> float:
    """
    è®¡ç®—åŠ æƒæ€»åˆ† (åŠ¨æ€æƒé‡ + ç›¸å…³æ€§ç†”æ–­ + è´Ÿé¢æƒ©ç½š)
    
    æ–°ç®—æ³•ç‰¹ç‚¹:
    1. ä½¿ç”¨ç™¾åˆ†æ¯”æƒé‡ï¼ˆæ€»å’Œä¸º1.0ï¼‰ï¼Œä¸åŒç±»å‹æ–‡ç« æƒé‡ä¸åŒ
    2. ç›¸å…³æ€§ç†”æ–­æœºåˆ¶ï¼šå¦‚æœç›¸å…³æ€§è¿‡ä½ï¼Œä¸€ç¥¨å¦å†³
    3. ä¼˜åŒ–çš„æƒ©ç½šæœºåˆ¶ï¼šSoft Flags æƒ©ç½šé™ä½è‡³0.5/é¡¹
    """
    # 1. è·å–æƒé‡é…ç½®ï¼ˆç°åœ¨æ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼‰
    weights_config = PROJ_CONFIG.get("scoring_weights", {})
    weights = weights_config.get(article_type, weights_config.get("default", {}))
    
    # 2. è®¡ç®—åŠ æƒåˆ†ï¼ˆç™¾åˆ†æ¯”æƒé‡ï¼Œæ€»å’Œä¸º1.0ï¼‰
    weighted_score = 0.0
    for key, score in scores.items():
        w = weights.get(key, 0.2)  # é»˜è®¤20%æƒé‡
        weighted_score += score * w
        logger.debug(f"  {key}: {score} Ã— {w} = {score * w}")
    
    logger.debug(f"åŸºç¡€åŠ æƒåˆ†: {weighted_score:.2f}")
    
    # 3. ç›¸å…³æ€§ç†”æ–­æœºåˆ¶ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
    relevance_threshold = PROJ_CONFIG.get("relevance_threshold", 2.5)
    relevance_score = scores.get("relevance", 5)
    
    if relevance_score < relevance_threshold:
        original_score = weighted_score
        weighted_score = min(weighted_score, relevance_threshold)
        logger.info(
            f"âš ï¸ ç›¸å…³æ€§ç†”æ–­è§¦å‘: relevance={relevance_score} < {relevance_threshold}, "
            f"æ€»åˆ†é™åˆ¶ {original_score:.1f} â†’ {weighted_score:.1f}"
        )
    
    # 4. è´Ÿé¢æ¸…å•å¤„ç†
    if red_flags:
        # Hard Flags: ç›´æ¥æ‰“å…¥å†·å®«ï¼ˆæœ€é«˜1.0åˆ†ï¼‰
        if any(flag in HARD_RED_FLAGS for flag in red_flags):
            logger.warning(f"ğŸš« Hard Flagè§¦å‘: {red_flags}, æ€»åˆ†é”å®šä¸º1.0")
            return 1.0
        
        # Soft Flags: æ¯é¡¹æ‰£0.5åˆ†ï¼ˆåŸæ¥æ˜¯1.0ï¼Œè¿‡äºä¸¥æ ¼ï¼‰
        penalty = len(red_flags) * 0.5
        original_score = weighted_score
        weighted_score = max(1.0, weighted_score - penalty)
        logger.info(f"ğŸš© Soft Flagsæƒ©ç½š: {red_flags}, æ‰£{penalty}åˆ†, {original_score:.1f} â†’ {weighted_score:.1f}")
        
    return round(weighted_score, 1)


def extract_json_from_response(response_text: str) -> str | None:
    """
    ä» LLM å“åº”ä¸­æå– JSONï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. çº¯ JSON å“åº”
    2. Markdown ä»£ç å—ä¸­çš„ JSON
    3. æ€ç»´é“¾åé¢è·Ÿç€çš„ JSON
    """
    # ç­–ç•¥1: å°è¯•æå– Markdown ä»£ç å—ä¸­çš„ JSON
    code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
    if code_block_match:
        return code_block_match.group(1)
    
    # ç­–ç•¥2: æ‰¾åˆ°æ‰€æœ‰å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œå–æœ€åä¸€ä¸ªï¼ˆé€šå¸¸æ€ç»´é“¾åœ¨å‰ï¼ŒJSONåœ¨åï¼‰
    # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œæ‰¾æ¯ä¸ªç‹¬ç«‹çš„ {...} å—
    json_objects = []
    depth = 0
    start_idx = None
    
    for i, char in enumerate(response_text):
        if char == '{':
            if depth == 0:
                start_idx = i
            depth += 1
        elif char == '}':
            depth -= 1
            if depth == 0 and start_idx is not None:
                candidate = response_text[start_idx:i+1]
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆ JSON
                try:
                    json.loads(candidate)
                    json_objects.append(candidate)
                except json.JSONDecodeError:
                    pass
                start_idx = None
    
    if json_objects:
        # ä¼˜å…ˆè¿”å›åŒ…å« "scores" å­—æ®µçš„ JSONï¼ˆè¿™æ˜¯æˆ‘ä»¬æœŸæœ›çš„æ ¼å¼ï¼‰
        for obj in reversed(json_objects):
            if '"scores"' in obj:
                return obj
        # å¦åˆ™è¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆ JSON
        return json_objects[-1]
    
    # ç­–ç•¥3: å›é€€åˆ°åŸæ¥çš„è´ªå©ªåŒ¹é…ï¼ˆå…¼å®¹æ€§ï¼‰
    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
    if json_match:
        return json_match.group()
    
    return None


def extract_json_array_from_response(response_text: str) -> str | None:
    """ä» LLM å“åº”ä¸­æå– JSON æ•°ç»„ï¼Œæ”¯æŒ markdown ä»£ç å—å’Œæ™ºèƒ½è¾¹ç•Œè¯†åˆ«ã€‚"""
    # ç­–ç•¥1: å°è¯•æå– Markdown ä»£ç å— (æ›´å®½æ¾çš„æ­£åˆ™)
    # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
    code_block_match = re.search(r'```(?:json)?\s*(.*?)```', response_text, re.DOTALL)
    if code_block_match:
        content = code_block_match.group(1).strip()
        if content.startswith('[') and content.endswith(']'):
            return content

    # ç­–ç•¥2: æ™ºèƒ½å¯»æ‰¾æœ€å¤–å±‚çš„ [] å¯¹ï¼Œä¸”å¿½ç•¥å­—ç¬¦ä¸²å†…çš„æ‹¬å·
    # è¿™æ¯”ç®€å•çš„æ·±åº¦è®¡æ•°æ›´å¥å£®ï¼Œèƒ½å¤„ç† [{"title": "æ–‡ç« [1]"}] è¿™ç§æƒ…å†µ

    candidates = []

    # å¯»æ‰¾æ‰€æœ‰çš„ '[' ä½œä¸ºæ½œåœ¨èµ·ç‚¹
    start_indices = [m.start() for m in re.finditer(r'\[', response_text)]

    for start in start_indices:
        depth = 0
        in_string = False
        escape = False

        for i in range(start, len(response_text)):
            char = response_text[i]

            if in_string:
                if escape:
                    escape = False
                elif char == '\\':
                    escape = True
                elif char == '"':
                    in_string = False
            else:
                if char == '"':
                    in_string = True
                elif char == '[':
                    depth += 1
                elif char == ']':
                    depth -= 1
                    if depth == 0:
                        # æ‰¾åˆ°é—­åˆçš„æ•°ç»„
                        candidate = response_text[start:i+1]
                        try:
                            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
                            json.loads(candidate)
                            candidates.append(candidate)
                            # æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„å°±è·³å‡ºå†…å±‚å¾ªç¯ï¼Œç»§ç»­æ‰¾ä¸‹ä¸€ä¸ªå¯èƒ½çš„èµ·ç‚¹ï¼ˆè™½ç„¶é€šå¸¸åªè¦æ‰¾åˆ°æœ€å¤§çš„é‚£ä¸ªå°±è¡Œï¼‰
                            break
                        except json.JSONDecodeError:
                            pass

    if candidates:
        # è¿”å›æœ€é•¿çš„é‚£ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€å¤–å±‚çš„ï¼‰
        return max(candidates, key=len)

    return None


def _score_from_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """ä»å·²è§£æçš„ JSON æ•°æ®ç”Ÿæˆè¯„åˆ†ç»“æœã€‚"""
    scores = data.get("scores", {})
    article_type = data.get("article_type", "default")
    red_flags = data.get("red_flags", [])
    analysis_text = data.get("analysis", "")

    overall_score = calculate_weighted_score(scores, article_type, red_flags)

    if overall_score >= 3.8:  # User feedback: 3.9 is also high quality
        verdict = "å€¼å¾—é˜…è¯»"
    elif overall_score >= 3.0:
        verdict = "ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»"
    else:
        verdict = "ä¸å€¼å¾—è¯»"

    if red_flags:
        verdict += f" (å« {', '.join(red_flags)})"

    return {
        "relevance_score": scores.get("relevance", 0),
        "informativeness_accuracy_score": scores.get("informativeness_accuracy", 0),
        "depth_opinion_score": scores.get("depth_opinion", 0),
        "readability_score": scores.get("readability", 0),
        "non_redundancy_score": scores.get("non_redundancy", 0),
        "overall_score": overall_score,
        "verdict": verdict,
        "reason": analysis_text,
        "comment": data.get("comment", analysis_text),
        "article_type": article_type,
        "red_flags": red_flags,
        "detailed_scores": data,
        "score": overall_score
    }


def parse_score_response(response_text: str) -> Dict[str, Any]:
    """è§£æå“åº”å¹¶è®¡ç®—æ€»åˆ†"""
    try:
        # ä½¿ç”¨æ”¹è¿›çš„ JSON æå–æ–¹æ³•
        json_str = extract_json_from_response(response_text)
        if json_str:
            data = json.loads(json_str)
            
            scores = data.get("scores", {})
            article_type = data.get("article_type", "default")
            red_flags = data.get("red_flags", [])
            analysis_text = data.get("analysis", "") # è·å–åˆ†ææ–‡æœ¬
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            overall_score = calculate_weighted_score(scores, article_type, red_flags)
            
            # ç”Ÿæˆä¸€å¥è¯ Verdict
            if overall_score >= 3.8:  # User feedback: 3.9 is also high quality
                verdict = "å€¼å¾—é˜…è¯»"
            elif overall_score >= 3.0:
                verdict = "ä¸€èˆ¬ï¼Œå¯é€‰"
            else:
                verdict = "ä¸å€¼å¾—è¯»"
                
            if red_flags:
                verdict += f" (å«: {', '.join(red_flags)})"

            return {
                "relevance_score": scores.get("relevance", 0),
                "informativeness_accuracy_score": scores.get("informativeness_accuracy", 0),
                "depth_opinion_score": scores.get("depth_opinion", 0),
                "readability_score": scores.get("readability", 0),
                "non_redundancy_score": scores.get("non_redundancy", 0),
                "overall_score": overall_score,
                "verdict": verdict,
                "reason": analysis_text, # ä½¿ç”¨ analysis å­—æ®µä½œä¸º reason
                "comment": data.get("comment", analysis_text), # å…¼å®¹ comment
                "article_type": article_type,
                "red_flags": red_flags,
                "detailed_scores": data # ä¿å­˜å®Œæ•´åŸå§‹æ•°æ®
            }
            
        return _default_error_result(f"æ— æ³•è§£æJSON: {response_text[:200]}")
    except json.JSONDecodeError as e:
        # JSONè§£æé”™è¯¯ï¼Œè®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        logger.error(f"åŸå§‹å“åº”å†…å®¹: {response_text[:500]}...")
        return _default_error_result(f"JSONè§£æé”™è¯¯: {e} | å“åº”ç‰‡æ®µ: {response_text[:100]}")
    except Exception as e:
        logger.error(f"è§£æå¤±è´¥: {e}")
        logger.error(f"åŸå§‹å“åº”å†…å®¹: {response_text[:500]}...")
        return _default_error_result(f"è§£æå¼‚å¸¸: {e}")


def _default_error_result(msg: str):
    return {
        "overall_score": 0.0,
        "verdict": "è§£æé”™è¯¯",
        "reason": msg,
        "comment": msg,
        "red_flags": [],
        "detailed_scores": {}
    }


def score_article(title: str, summary: str, content: str) -> Dict[str, Any]:
    """
    å¯¹æ–‡ç« è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
    """
    try:
        analysis_profile = PROJ_CONFIG.get("analysis_profile")
        
        client = OpenAI(
            api_key=get_config("OPENAI_API_KEY", profile=analysis_profile),
            base_url=get_config("OPENAI_BASE_URL", profile=analysis_profile)
        )
        
        prompt = build_scoring_prompt(title, summary, content)
        log_debug("Scoring Prompt", prompt)
        
        response = client.chat.completions.create(
            model=get_config("OPENAI_MODEL", "gpt-4o-mini", profile=analysis_profile),
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # ç¨å¾®é™ä½ä¸€ç‚¹ï¼Œæ›´ç¨³å®š
            max_tokens=1500   # å¢åŠ  Token ä¸Šé™ä»¥å®¹çº³ Analysis
        )
        
        response_text = response.choices[0].message.content
        log_debug("Scoring Response", response_text)
        
        if not response_text:
            return _default_error_result("æ¨¡å‹è¿”å›ä¸ºç©º")
        
        result = parse_score_response(response_text)
        
        # è¡¥å…¨ score å­—æ®µï¼Œå…¼å®¹æ—§çš„ article_analyzer è°ƒç”¨
        result['score'] = result['overall_score']
        
        return result
        
    except Exception as e:
        logger.error(f"è¯„åˆ†è¿‡ç¨‹å¼‚å¸¸: {e}")
        return _default_error_result(f"Exception: {str(e)}")


def format_score_result(score_result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å±•ç¤º"""
    verdict = score_result.get("verdict", "æœªçŸ¥")
    overall = score_result.get("overall_score", 0.0)
    
    emoji = "ğŸ˜"
    if overall >= 3.8:
        emoji = "ğŸ”¥"
    elif overall <= 2.0:
        emoji = "ğŸ—‘ï¸" # åƒåœ¾æ¡¶
    elif overall < 3.0:
        emoji = "ğŸ‘"
    
    return f"{emoji} {verdict} ({overall}/5.0)"


def parse_batch_score_response(response_text: str, expected_count: int) -> list[Dict[str, Any]] | None:
    """
    è§£ææ‰¹é‡è¯„åˆ†å“åº”å¹¶è¿”å›ç»“æœåˆ—è¡¨ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    æ”¹è¿›é€»è¾‘ï¼š
    1. å³ä½¿ JSON æ•°ç»„æœªé—­åˆï¼Œä¹Ÿå°è¯•æå–å·²è§£æçš„å¯¹è±¡ã€‚
    2. å¦‚æœæ•°é‡ä¸è¶³ï¼Œè¿”å›éƒ¨åˆ†ç»“æœï¼ˆåˆ—è¡¨é•¿åº¦ < expected_countï¼‰ï¼Œç”±è°ƒç”¨è€…è´Ÿè´£è¡¥å…¨ã€‚
    """
    try:
        # 1. å°è¯•å®Œæ•´è§£æ
        json_str = extract_json_array_from_response(response_text)
        data = None
        if json_str:
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                pass

        # 2. å¦‚æœå®Œæ•´è§£æå¤±è´¥ï¼Œå°è¯•æµå¼æå–ï¼ˆåº”å¯¹æˆªæ–­æƒ…å†µï¼‰
        if not data:
            data = _robust_parse_objects(response_text)

        if not data or not isinstance(data, list):
            logger.warning("Batch parse failed: No valid objects found")
            return None

        results_by_index = {}
        for item in data:
            if not isinstance(item, dict):
                continue
            index = item.get("index")
            score_result = _score_from_data(item)
            if index is not None:
                results_by_index[index] = score_result

        if not results_by_index:
            return None

        # 3. æ„é€ æœ‰åºåˆ—è¡¨ï¼ˆåŒ…å« None å ä½ç¬¦ï¼‰
        ordered = []
        valid_count = 0
        for i in range(expected_count):
            if i in results_by_index:
                ordered.append(results_by_index[i])
                valid_count += 1
            else:
                ordered.append(None) # æ ‡è®°ä¸ºç¼ºå¤±

        if valid_count == 0:
            return None

        # åªè¦æœ‰éƒ¨åˆ†æˆåŠŸï¼Œå°±è¿”å›è¿™ä¸ªåŒ…å« None çš„åˆ—è¡¨
        return ordered

    except Exception as e:
        logger.error(f"æ‰¹é‡è§£æå¤±è´¥: {e}")
        return None


def _robust_parse_objects(text: str) -> list[dict]:
    """
    ä»å¯èƒ½æˆªæ–­çš„æ–‡æœ¬ä¸­å°½å¯èƒ½å¤šåœ°æå– JSON å¯¹è±¡ã€‚
    åŸç†ï¼šæŸ¥æ‰¾æ‰€æœ‰ {"index": ...} æ¨¡å¼çš„å—ï¼Œå°è¯•è§£æã€‚
    """
    objects = []
    # æŸ¥æ‰¾æ‰€æœ‰æ½œåœ¨çš„å¯¹è±¡èµ·å§‹ç‚¹
    # å‡è®¾æ¯ä¸ªå¯¹è±¡éƒ½ä»¥ {"index": å¼€å¤´ï¼ˆå— prompt çº¦æŸï¼‰
    # æˆ–è€…é€šç”¨çš„ { ... }

    # ç®€å•ç­–ç•¥ï¼šæŒ‰ '{"index":' åˆ†å‰²ï¼Œæˆ–è€…ä½¿ç”¨éè´ªå©ªæ­£åˆ™æå–
    # æ³¨æ„ï¼šæ­£åˆ™å¯èƒ½æ— æ³•å¤„ç†åµŒå¥—å¤§æ‹¬å·ï¼Œæ‰€ä»¥è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ ˆå¼è§£æå™¨æ‰«æ

    start_indices = [m.start() for m in re.finditer(r'\{\s*"index"', text)]

    for start in start_indices:
        # å°è¯•ä» start å¼€å§‹å‘åå¯»æ‰¾åˆæ³•çš„é—­åˆ }
        depth = 0
        in_string = False
        escape = False

        for i in range(start, len(text)):
            char = text[i]
            if in_string:
                if escape: escape = False
                elif char == '\\': escape = True
                elif char == '"': in_string = False
            else:
                if char == '"': in_string = True
                elif char == '{': depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        # æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡å€™é€‰
                        candidate = text[start:i+1]
                        try:
                            obj = json.loads(candidate)
                            objects.append(obj)
                        except:
                            pass
                        break # è·³å‡ºå†…å±‚å¾ªç¯ï¼Œå¤„ç†ä¸‹ä¸€ä¸ª start
    return objects


def score_articles_batch(articles: list[dict], max_retries: int = 3) -> list[Dict[str, Any]] | None:
    """å¯¹å¤šç¯‡æ–‡ç« è¿›è¡Œæ‰¹é‡è¯„åˆ†ï¼Œæ”¯æŒé‡è¯•å’Œéƒ¨åˆ†æ¢å¤ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚"""
    analysis_profile = PROJ_CONFIG.get("analysis_profile")

    try:
        client = OpenAI(
            api_key=get_config("OPENAI_API_KEY", profile=analysis_profile),
            base_url=get_config("OPENAI_BASE_URL", profile=analysis_profile)
        )
    except Exception as e:
        logger.error(f"Client init failed: {e}")
        return None

    prompt = build_batch_scoring_prompt(articles)
    log_debug("Batch Scoring Prompt", prompt)

    for attempt in range(max_retries):
        try:
            logger.info(f"Batch scoring attempt {attempt + 1}/{max_retries}...")

            response = client.chat.completions.create(
                model=get_config("OPENAI_MODEL", "gpt-4o-mini", profile=analysis_profile),
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=16000  # Explicitly set high limit for Gemini
            )

            response_text = response.choices[0].message.content
            log_debug(f"Batch Scoring Response (Attempt {attempt + 1})", response_text)

            if not response_text:
                logger.warning(f"Batch attempt {attempt + 1} failed: Empty response")
                continue

            # è§£æç»“æœï¼ˆå¯èƒ½åŒ…å« Noneï¼‰
            batch_results = parse_batch_score_response(response_text, len(articles))

            if batch_results:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±é¡¹ (None)
                missing_indices = [i for i, r in enumerate(batch_results) if r is None]

                if not missing_indices:
                    return batch_results # å®Œç¾æˆåŠŸ

                logger.warning(f"Batch attempt {attempt + 1} partial success. Missing indices: {missing_indices}. Filling gaps...")

                # è¡¥å…¨ç¼ºå¤±é¡¹ï¼ˆå•ç¯‡è°ƒç”¨ï¼‰
                for i in missing_indices:
                    logger.info(f"Filling gap for article {i} (single mode)...")
                    article = articles[i]
                    # ä½¿ç”¨å•ç¯‡è¯„åˆ†è¡¥å…¨
                    try:
                        score_result = score_article(
                            article.get("title", ""),
                            article.get("summary", ""),
                            article.get("content", "")
                        )
                        batch_results[i] = score_result
                    except Exception as e:
                        logger.error(f"Failed to fill gap for article {i}: {e}")
                        # ä¿æŒ None æˆ–è€…å¡«å…¥é”™è¯¯å ä½ç¬¦ï¼Œè¿™é‡Œå¡«å…¥é»˜è®¤é”™è¯¯
                        batch_results[i] = _default_error_result(f"Fill gap failed: {e}")

                return batch_results

            logger.warning(f"Batch attempt {attempt + 1} failed: Parse error or no valid objects found")
            logger.warning(f"Response length: {len(response_text)} chars")
            logger.warning(f"Response starts with: {response_text[:200] if response_text else 'EMPTY'}")
            logger.warning(f"Response ends with: {response_text[-200:] if response_text else 'EMPTY'}")

        except Exception as e:
            logger.warning(f"Batch attempt {attempt + 1} exception: {e}")

    logger.error("All batch scoring attempts failed.")
    return None
