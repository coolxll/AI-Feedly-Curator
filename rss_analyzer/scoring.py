"""
æ–‡ç« è¯„åˆ†æ¨¡å—
åŸºäºå¤šç»´åº¦è¯„ä¼°æ–‡ç« çš„é˜…è¯»ä»·å€¼
"""
import json
import re
import logging
from typing import Dict, Any
from datetime import datetime

from openai import OpenAI

from .config import PROJ_CONFIG, get_config, log_debug

logger = logging.getLogger(__name__)


# è¯„åˆ†ç»´åº¦é…ç½® (ä¿ç•™ Metadataï¼Œæƒé‡ç°åœ¨æ˜¯åŠ¨æ€çš„)
SCORING_DIMENSIONS = {
    "relevance": {"name": "ç›¸å…³æ€§"},
    "informativeness_accuracy": {"name": "ä¿¡æ¯é‡ä¸å‡†ç¡®æ€§"},
    "depth_opinion": {"name": "æ·±åº¦ä¸è§‚ç‚¹"},
    "readability": {"name": "å¯è¯»æ€§"},
    "non_redundancy": {"name": "åŸåˆ›æ€§/æ°´åˆ†åº¦"}
}

# åŠ¨æ€æƒé‡é…ç½®
DEFAULT_WEIGHTS = PROJ_CONFIG.get("scoring_weights", {}).get("default", {
    "relevance": 2, "informativeness_accuracy": 2, "depth_opinion": 2, "readability": 2, "non_redundancy": 1
})

# è´Ÿé¢æ¸…å•é…ç½®
RED_FLAGS = [
    "pure_promotion",  # çº¯æ¨å¹¿/è½¯æ–‡ (Soft)
    "clickbait",       # æ ‡é¢˜å…š (Soft)
    "ai_generated"     # AI ç”Ÿæˆæ„Ÿè¿‡é‡/é€»è¾‘æ··ä¹± (Hard)
]

HARD_RED_FLAGS = {"ai_generated"}


def build_scoring_prompt(title: str, summary: str, content: str) -> str:
    """æ„å»ºç»“æ„åŒ–è¯„åˆ†æç¤ºè¯ (å«æ€ç»´é“¾ & æ™ºèƒ½æˆªæ–­)"""
    persona = PROJ_CONFIG.get("scoring_persona", "")
    
    # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å¤´éƒ¨å’Œå°¾éƒ¨ï¼Œä¸­é—´æˆªæ–­
    if len(content) > 10000:
        content_snippet = content[:6000] + "\n\n...[å†…å®¹è¿‡é•¿ï¼Œä¸­é—´éƒ¨åˆ†çœç•¥]...\n\n" + content[-3000:]
    else:
        content_snippet = content

    today_str = datetime.now().strftime("%Y-%m-%d")

    return f"""{persona}

å½“å‰æ—¥æœŸï¼š{today_str}

è¯·æ ¹æ®ä½ çš„ä¸“ä¸šèƒŒæ™¯ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–‡ç« è¿›è¡Œæ·±åº¦è¯„ä¼°ï¼š

### ç¬¬ä¸€æ­¥ï¼šåˆ†æä¸æ€è€ƒ (Chain of Thought)
è¯·å…ˆé€šè¯»å…¨æ–‡ï¼Œ**é¦–å…ˆåˆ¤æ–­æ–‡ç« ç±»å‹**ï¼Œç„¶åæ ¹æ®ç±»å‹ç‰¹ç‚¹è¿›è¡Œè¯„ä¼°ï¼š

**ç±»å‹è¯†åˆ«æŒ‡å—**ï¼š
- **newsï¼ˆèµ„è®¯ï¼‰**: æŠ¥é“äº‹ä»¶ã€æ•°æ®ã€å‘å¸ƒæ¶ˆæ¯ï¼Œé‡ç‚¹æ˜¯â€œæ˜¯ä»€ä¹ˆâ€
- **tutorialï¼ˆæ•™ç¨‹ï¼‰**: æ•™æˆæŠ€èƒ½ã€å±•ç¤ºæ­¥éª¤ã€æä¾›ä»£ç ï¼Œé‡ç‚¹æ˜¯â€œæ€ä¹ˆåšâ€
- **opinionï¼ˆè§‚ç‚¹ï¼‰**: åˆ†æé—®é¢˜ã€è¯„è®ºç°è±¡ã€æå‡ºè§è§£ï¼Œé‡ç‚¹æ˜¯â€œä¸ºä»€ä¹ˆâ€

**é’ˆå¯¹æ€§åˆ†æ**ï¼š
- èµ„è®¯ç±»ï¼šå…³æ³¨æ—¶æ•ˆæ€§ã€å‡†ç¡®æ€§ã€æ•°æ®å®Œæ•´æ€§
- æ•™ç¨‹ç±»ï¼šå…³æ³¨å¯å¤ç°æ€§ã€æ­¥éª¤æ¸…æ™°åº¦ã€å®é™…è§£å†³é—®é¢˜
- è§‚ç‚¹ç±»ï¼šå…³æ³¨è®ºè¯æ·±åº¦ã€ç‹¬åˆ°è§è§£ã€é€»è¾‘è‡ªæ´½

é€šç”¨æ£€æŸ¥ï¼š
- æ˜¯å¦æœ‰è¿‡åº¦çš„è¥é”€è¯æœ¯æˆ–è¯¯å¯¼æ€§æ ‡é¢˜ï¼Ÿ
- æ˜¯å¦æœ‰æ˜æ˜¾çš„AIç”Ÿæˆç—•è¿¹æˆ–é€»è¾‘æ··ä¹±ï¼Ÿ

### ç¬¬äºŒæ­¥ï¼šç±»å‹åˆ¤æ–­ & è´Ÿé¢æ£€æµ‹
1. **åˆ¤æ–­æ–‡ç« ç±»å‹**ï¼š`news` (èµ„è®¯), `tutorial` (æ•™ç¨‹), `opinion` (è§‚ç‚¹).
2. **æ£€æµ‹è´Ÿé¢ç‰¹å¾**ï¼š
   - `pure_promotion`: çº¯å¹¿å‘Š/å–è¯¾/æ¨é”€äº§å“ (Soft Flag). **æ³¨æ„ï¼šæŠ€æœ¯è§†è§’çš„å·¥å…·æ¨èã€æ–°åŠŸèƒ½å‘å¸ƒã€å¼€æºé¡¹ç›®ä»‹ç»ã€æŠ•è¡ŒæŠ¥å‘Šæ‘˜è¦å‡ä¸ç®—å¹¿å‘Šã€‚**
   - `clickbait`: æ ‡é¢˜å…š (Soft Flag)
   - `ai_generated`: æ˜æ˜¾çš„ AI ç”Ÿæˆç—•è¿¹/é€»è¾‘æ··ä¹± (Hard Flag)

### ç¬¬ä¸‰æ­¥ï¼šå¤šç»´åº¦æ‰“åˆ† (1-5åˆ†ï¼Œæ ¹æ®ç±»å‹çµæ´»è¯„åˆ†)

> **é‡è¦ï¼šä¸åŒç±»å‹æ–‡ç« çš„è¯„åˆ†ä¾§é‡ç‚¹ä¸åŒï¼**

#### é€šç”¨è¯„åˆ†æ ‡å‡†
- 5åˆ†ï¼šæä½³
- 4åˆ†ï¼šä¼˜ç§€
- 3åˆ†ï¼šåŠæ ¼ï¼ˆå¤§éƒ¨åˆ†æ–‡ç« åœ¨æ­¤åŒºé—´ï¼‰
- 1-2åˆ†ï¼šå·®

#### ç»´åº¦è¯¦è§£ï¼ˆè¯·æ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´è¯„åˆ†é‡ç‚¹ï¼‰

1. **ç›¸å…³æ€§**ï¼ˆæ‰€æœ‰ç±»å‹çš„é¦–è¦ç»´åº¦ï¼‰
   - æ˜¯å¦ç¬¦åˆç”¨æˆ·äººè®¾ï¼ˆTech / æŠ•èµ„ / å›½é™…æ”¿æ²»ï¼‰
   - âš ï¸ å¦‚æœç›¸å…³æ€§ < 2.5ï¼Œå³ä½¿å…¶ä»–ç»´åº¦å†é«˜ï¼Œæ–‡ç« ä¹Ÿä¸æ¨è

2. **ä¿¡æ¯é‡ä¸å‡†ç¡®æ€§**
   - **news**: æ ¸å¿ƒç»´åº¦ï¼æ•°æ®æ˜¯å¦å®Œæ•´ã€æ¥æºæ˜¯å¦å¯é ã€æ˜¯å¦æœ‰æ–°å¢ä¿¡æ¯
   - **tutorial**: æ­¥éª¤æ˜¯å¦è¯¦å°½ã€ä»£ç æ˜¯å¦å¯ç”¨ã€æ˜¯å¦è§£å†³çœŸå®é—®é¢˜
   - **opinion**: è®ºæ®æ˜¯å¦å……åˆ†ã€äº‹å®æ˜¯å¦å‡†ç¡®

3. **æ·±åº¦ä¸è§‚ç‚¹**ï¼ˆæƒé‡å› ç±»å‹è€Œå¼‚ï¼‰
   - **news**: âš ï¸ ä¸å¼ºæ±‚æ·±åº¦åˆ†æï¼å¦‚æœæ˜¯åŠæ—¶ã€å‡†ç¡®çš„å¸‚åœºåŠ¨æ€/è¡Œä¸šèµ„è®¯ï¼Œå³ä½¿åªæ˜¯æ•°æ®æ±‡æ€»ï¼Œä¹Ÿåº”ç»™ â‰¥3åˆ†
   - **tutorial**: æ˜¯å¦åŸºäºå®è·µç»éªŒã€æ˜¯å¦è¦†ç›–å¸¸è§å‘ç‚¹
   - **opinion**: æ ¸å¿ƒç»´åº¦ï¼æ˜¯å¦æœ‰ç‹¬åˆ°è§è§£ã€è®ºè¯æ˜¯å¦æ·±åˆ»

4. **å¯è¯»æ€§**
   - ç»“æ„æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦é¡ºç•…
   - tutorialç±»ï¼šæ­¥éª¤ç¼–å·ã€ä»£ç æ ¼å¼æ˜¯å¦æ¸…æ™°ï¼ˆæƒé‡æ›´é«˜ï¼‰

5. **åŸåˆ›æ€§/æ°´åˆ†åº¦**
   - news: æ˜¯å¦é¦–å‘ã€æ˜¯å¦æ´—ç¨¿
   - tutorial: âš ï¸ å¯¹äºtroubleshootingï¼ˆæ’é”™ï¼‰ç±»æ–‡ç« ï¼Œå³ä½¿æ–¹æ¡ˆå¸¸è§ä½†åªè¦è§£å†³äº†å…·ä½“é—®é¢˜ï¼Œä¹Ÿåº”ç»™ â‰¥3åˆ†
   - opinion: æ˜¯å¦æ‹„è¢–ã€æ˜¯å¦æ³›æ³›è€Œè°ˆ

### è¾“å‡ºæ ¼å¼ (JSON Only)
è¯·åªè¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ï¼š
{{
  "analysis": "1-2å¥è¯çš„ç®€è¦åˆ†æï¼Œè¯´æ˜æ‰“åˆ†ç†ç”±",
  "article_type": "news/tutorial/opinion",
  "red_flags": ["clickbait", ...],  // æ— åˆ™ä¸ºç©ºæ•°ç»„
  "scores": {{
    "relevance": <1-5>,
    "informativeness_accuracy": <1-5>,
    "depth_opinion": <1-5>,
    "readability": <1-5>,
    "non_redundancy": <1-5>
  }}
}}

---
æ–‡ç« ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary[:200] if summary else 'æ— '}
æ­£æ–‡ï¼š
{content_snippet}
"""


def calculate_weighted_score(scores: Dict[str, int], article_type: str, red_flags: list) -> float:
    """
    è®¡ç®—åŠ æƒæ€»åˆ† (åŠ¨æ€æƒé‡ + ç›¸å…³æ€§ç†”æ–­ + è´Ÿé¢æƒ©ç½š)
    
    æ–°ç®—æ³•ç‰¹ç‚¹:
    1. ä½¿ç”¨ç™¾åˆ†æ¯”æƒé‡ï¼ˆæ€»å’Œä¸º1.0ï¼‰ï¼Œä¸åŒç±»å‹æ–‡ç« æƒé‡ä¸åŒ
    2. ç›¸å…³æ€§ç†”æ–­æœºåˆ¶ï¼šå¦‚æœç›¸å…³æ€§è¿‡ä½ï¼Œä¸€ç¥¨å¦å†³
    3. ä¼˜åŒ–çš„æƒ©ç½šæœºåˆ¶ï¼šSoft Flags æƒ©ç½šé™ä½è‡³0.5/é¡¹
    """
    # 1. è·å–æƒé‡é…ç½®ï¼ˆç°åœ¨æ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼‰
    weights_config = PROJ_CONFIG.get("scoring_weights", {})
    weights = weights_config.get(article_type, weights_config.get("default", {}))
    
    # 2. è®¡ç®—åŠ æƒåˆ†ï¼ˆç™¾åˆ†æ¯”æƒé‡ï¼Œæ€»å’Œä¸º1.0ï¼‰
    weighted_score = 0.0
    for key, score in scores.items():
        w = weights.get(key, 0.2)  # é»˜è®¤20%æƒé‡
        weighted_score += score * w
        logger.debug(f"  {key}: {score} Ã— {w} = {score * w}")
    
    logger.debug(f"åŸºç¡€åŠ æƒåˆ†: {weighted_score:.2f}")
    
    # 3. ç›¸å…³æ€§ç†”æ–­æœºåˆ¶ï¼ˆä¸€ç¥¨å¦å†³ï¼‰
    relevance_threshold = PROJ_CONFIG.get("relevance_threshold", 2.5)
    relevance_score = scores.get("relevance", 5)
    
    if relevance_score < relevance_threshold:
        original_score = weighted_score
        weighted_score = min(weighted_score, relevance_threshold)
        logger.info(
            f"âš ï¸ ç›¸å…³æ€§ç†”æ–­è§¦å‘: relevance={relevance_score} < {relevance_threshold}, "
            f"æ€»åˆ†é™åˆ¶ {original_score:.1f} â†’ {weighted_score:.1f}"
        )
    
    # 4. è´Ÿé¢æ¸…å•å¤„ç†
    if red_flags:
        # Hard Flags: ç›´æ¥æ‰“å…¥å†·å®«ï¼ˆæœ€é«˜1.0åˆ†ï¼‰
        if any(flag in HARD_RED_FLAGS for flag in red_flags):
            logger.warning(f"ğŸš« Hard Flagè§¦å‘: {red_flags}, æ€»åˆ†é”å®šä¸º1.0")
            return 1.0
        
        # Soft Flags: æ¯é¡¹æ‰£0.5åˆ†ï¼ˆåŸæ¥æ˜¯1.0ï¼Œè¿‡äºä¸¥æ ¼ï¼‰
        penalty = len(red_flags) * 0.5
        original_score = weighted_score
        weighted_score = max(1.0, weighted_score - penalty)
        logger.info(f"ğŸš© Soft Flagsæƒ©ç½š: {red_flags}, æ‰£{penalty}åˆ†, {original_score:.1f} â†’ {weighted_score:.1f}")
        
    return round(weighted_score, 1)


def extract_json_from_response(response_text: str) -> str | None:
    """
    ä» LLM å“åº”ä¸­æå– JSONï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. çº¯ JSON å“åº”
    2. Markdown ä»£ç å—ä¸­çš„ JSON
    3. æ€ç»´é“¾åé¢è·Ÿç€çš„ JSON
    """
    # ç­–ç•¥1: å°è¯•æå– Markdown ä»£ç å—ä¸­çš„ JSON
    code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
    if code_block_match:
        return code_block_match.group(1)
    
    # ç­–ç•¥2: æ‰¾åˆ°æ‰€æœ‰å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œå–æœ€åä¸€ä¸ªï¼ˆé€šå¸¸æ€ç»´é“¾åœ¨å‰ï¼ŒJSONåœ¨åï¼‰
    # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œæ‰¾æ¯ä¸ªç‹¬ç«‹çš„ {...} å—
    json_objects = []
    depth = 0
    start_idx = None
    
    for i, char in enumerate(response_text):
        if char == '{':
            if depth == 0:
                start_idx = i
            depth += 1
        elif char == '}':
            depth -= 1
            if depth == 0 and start_idx is not None:
                candidate = response_text[start_idx:i+1]
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆ JSON
                try:
                    json.loads(candidate)
                    json_objects.append(candidate)
                except json.JSONDecodeError:
                    pass
                start_idx = None
    
    if json_objects:
        # ä¼˜å…ˆè¿”å›åŒ…å« "scores" å­—æ®µçš„ JSONï¼ˆè¿™æ˜¯æˆ‘ä»¬æœŸæœ›çš„æ ¼å¼ï¼‰
        for obj in reversed(json_objects):
            if '"scores"' in obj:
                return obj
        # å¦åˆ™è¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆ JSON
        return json_objects[-1]
    
    # ç­–ç•¥3: å›é€€åˆ°åŸæ¥çš„è´ªå©ªåŒ¹é…ï¼ˆå…¼å®¹æ€§ï¼‰
    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
    if json_match:
        return json_match.group()
    
    return None


def parse_score_response(response_text: str) -> Dict[str, Any]:
    """è§£æå“åº”å¹¶è®¡ç®—æ€»åˆ†"""
    try:
        # ä½¿ç”¨æ”¹è¿›çš„ JSON æå–æ–¹æ³•
        json_str = extract_json_from_response(response_text)
        if json_str:
            data = json.loads(json_str)
            
            scores = data.get("scores", {})
            article_type = data.get("article_type", "default")
            red_flags = data.get("red_flags", [])
            analysis_text = data.get("analysis", "") # è·å–åˆ†ææ–‡æœ¬
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            overall_score = calculate_weighted_score(scores, article_type, red_flags)
            
            # ç”Ÿæˆä¸€å¥è¯ Verdict
            if overall_score >= 3.8:  # User feedback: 3.9 is also high quality
                verdict = "å€¼å¾—é˜…è¯»"
            elif overall_score >= 3.0:
                verdict = "ä¸€èˆ¬ï¼Œå¯é€‰"
            else:
                verdict = "ä¸å€¼å¾—è¯»"
                
            if red_flags:
                verdict += f" (å«: {', '.join(red_flags)})"

            return {
                "relevance_score": scores.get("relevance", 0),
                "informativeness_accuracy_score": scores.get("informativeness_accuracy", 0),
                "depth_opinion_score": scores.get("depth_opinion", 0),
                "readability_score": scores.get("readability", 0),
                "non_redundancy_score": scores.get("non_redundancy", 0),
                "overall_score": overall_score,
                "verdict": verdict,
                "reason": analysis_text, # ä½¿ç”¨ analysis å­—æ®µä½œä¸º reason
                "comment": data.get("comment", analysis_text), # å…¼å®¹ comment
                "article_type": article_type,
                "red_flags": red_flags,
                "detailed_scores": data # ä¿å­˜å®Œæ•´åŸå§‹æ•°æ®
            }
            
        return _default_error_result(f"æ— æ³•è§£æJSON: {response_text[:200]}")
    except json.JSONDecodeError as e:
        # JSONè§£æé”™è¯¯ï¼Œè®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        logger.error(f"åŸå§‹å“åº”å†…å®¹: {response_text[:500]}...")
        return _default_error_result(f"JSONè§£æé”™è¯¯: {e} | å“åº”ç‰‡æ®µ: {response_text[:100]}")
    except Exception as e:
        logger.error(f"è§£æå¤±è´¥: {e}")
        logger.error(f"åŸå§‹å“åº”å†…å®¹: {response_text[:500]}...")
        return _default_error_result(f"è§£æå¼‚å¸¸: {e}")


def _default_error_result(msg: str):
    return {
        "overall_score": 0.0,
        "verdict": "è§£æé”™è¯¯",
        "reason": msg,
        "comment": msg,
        "red_flags": [],
        "detailed_scores": {}
    }


def score_article(title: str, summary: str, content: str) -> Dict[str, Any]:
    """
    å¯¹æ–‡ç« è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
    """
    try:
        analysis_profile = PROJ_CONFIG.get("analysis_profile")
        
        client = OpenAI(
            api_key=get_config("OPENAI_API_KEY", profile=analysis_profile),
            base_url=get_config("OPENAI_BASE_URL", profile=analysis_profile)
        )
        
        prompt = build_scoring_prompt(title, summary, content)
        log_debug("Scoring Prompt", prompt)
        
        response = client.chat.completions.create(
            model=get_config("OPENAI_MODEL", "gpt-4o-mini", profile=analysis_profile),
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # ç¨å¾®é™ä½ä¸€ç‚¹ï¼Œæ›´ç¨³å®š
            max_tokens=1500   # å¢åŠ  Token ä¸Šé™ä»¥å®¹çº³ Analysis
        )
        
        response_text = response.choices[0].message.content
        log_debug("Scoring Response", response_text)
        
        if not response_text:
            return _default_error_result("æ¨¡å‹è¿”å›ä¸ºç©º")
        
        result = parse_score_response(response_text)
        
        # è¡¥å…¨ score å­—æ®µï¼Œå…¼å®¹æ—§çš„ article_analyzer è°ƒç”¨
        result['score'] = result['overall_score']
        
        return result
        
    except Exception as e:
        logger.error(f"è¯„åˆ†è¿‡ç¨‹å¼‚å¸¸: {e}")
        return _default_error_result(f"Exception: {str(e)}")


def format_score_result(score_result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å±•ç¤º"""
    verdict = score_result.get("verdict", "æœªçŸ¥")
    overall = score_result.get("overall_score", 0.0)
    
    emoji = "ğŸ˜"
    if overall >= 3.8:
        emoji = "ğŸ”¥"
    elif overall <= 2.0:
        emoji = "ğŸ—‘ï¸" # åƒåœ¾æ¡¶
    elif overall < 3.0:
        emoji = "ğŸ‘"
    
    return f"{emoji} {verdict} ({overall}/5.0)"
