"""
æ–‡ç« è¯„åˆ†æ¨¡å—
åŸºäºŽå¤šç»´åº¦è¯„ä¼°æ–‡ç« çš„é˜…è¯»ä»·å€¼
"""
import json
import re
import logging
from typing import Dict, Any

from openai import OpenAI

from .config import PROJ_CONFIG, get_config, log_debug

logger = logging.getLogger(__name__)


# è¯„åˆ†ç»´åº¦é…ç½®
SCORING_DIMENSIONS = {
    "relevance": {"name": "ç›¸å…³æ€§", "max": 5},
    "informativeness_accuracy": {"name": "ä¿¡æ¯é‡ä¸Žå‡†ç¡®æ€§", "max": 5},
    "depth_opinion": {"name": "æ·±åº¦ä¸Žè§‚ç‚¹", "max": 5},
    "readability": {"name": "å¯è¯»æ€§", "max": 5},
    "non_redundancy": {"name": "åŽŸåˆ›æ€§/æ°´åˆ†åº¦", "max": 5}
}

# è¯„åˆ†åˆ†ç±»
SCORE_CATEGORIES = {
    "must_read": {"min": 4.0, "label": "å€¼å¾—é˜…è¯»", "emoji": "ðŸ”¥"},
    "optional": {"min": 3.0, "label": "ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»", "emoji": "ðŸ˜"},
    "skip": {"min": 0.0, "label": "ä¸å¤ªå€¼å¾—é˜…è¯»", "emoji": "ðŸ‘Ž"}
}


def build_scoring_prompt(title: str, summary: str, content: str) -> str:
    """
    æž„å»ºç»“æž„åŒ–è¯„åˆ†æç¤ºè¯
    """
    persona = PROJ_CONFIG.get("scoring_persona", "")
    
    return f"""{persona}

è¯·æ ¹æ®ä½ çš„ä¸“ä¸šèƒŒæ™¯å’Œåå¥½ï¼Œä»¥åŠä¸‹é¢çš„æ ‡å‡†ï¼Œåˆ¤æ–­ç»™å®šæ–‡ç« æ˜¯å¦å€¼å¾—ä½ èŠ±æ—¶é—´å®Œæ•´é˜…è¯»ã€‚

è¯„åˆ†ç»´åº¦ï¼š

1. ç›¸å…³æ€§ï¼ˆ1â€“5 åˆ†ï¼‰ï¼šæ–‡ç« å†…å®¹æ˜¯å¦ç´§æ‰£ã€æµ‹è¯•å¼€å‘ã€DevOpsã€AIç¼–ç¨‹ã€Vibe Codingã€‘ç­‰ä½ çš„æ ¸å¿ƒå…´è¶£ç‚¹ã€‚

2. ä¿¡æ¯é‡ä¸Žå‡†ç¡®æ€§ï¼ˆ1â€“5 åˆ†ï¼‰ï¼šæ˜¯å¦æä¾›æ–°çš„å·¥å…·ã€æ¡†æž¶ã€æ–¹æ³•è®ºï¼Œæˆ–å¯¹çŽ°æœ‰æŠ€æœ¯æœ‰ç‹¬åˆ°è§è§£ã€‚

3. æ·±åº¦ä¸Žè§‚ç‚¹ï¼ˆ1â€“5 åˆ†ï¼‰ï¼šæ˜¯å¦æœ‰æŠ€æœ¯æ·±åº¦ï¼Œèƒ½å¦å¯å‘æ€è€ƒï¼Œè€Œä¸æ˜¯ç®€å•çš„å…¥é—¨æ•™ç¨‹æˆ–æ–‡æ¡£ç¿»è¯‘ã€‚

4. å¯è¯»æ€§ï¼ˆ1â€“5 åˆ†ï¼‰ï¼šä»£ç ç¤ºä¾‹æ˜¯å¦æ¸…æ™°ï¼Œé€»è¾‘æ˜¯å¦é¡ºç•…ï¼Œè¯»èµ·æ¥æ˜¯å¦äº«å—ï¼ˆç¬¦åˆ Vibe Coding çš„ç¾Žæ„Ÿï¼‰ã€‚

5. é‡å¤åº¦/æ°´åˆ†ï¼ˆ1â€“5 åˆ†ï¼‰ï¼šåˆ†æ•°è¶Šé«˜è¡¨ç¤º"æ°´åˆ†è¶Šå°‘"ã€‚æ‹’ç»è¥é”€è½¯æ–‡ã€æ— æ„ä¹‰çš„ç„¦è™‘è´©å–ã€‚

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š

1. å…ˆæ ¹æ®äº”ä¸ªç»´åº¦åˆ†åˆ«æ‰“ 1â€“5 åˆ†ã€‚
2. è®¡ç®—ä¸€ä¸ªæ€»åˆ†ï¼ˆå–äº”ä¸ªç»´åº¦çš„å¹³å‡åˆ†ï¼Œä¿ç•™ä¸€ä½å°æ•°ï¼‰ã€‚
3. æ ¹æ®æ€»åˆ†ç»™å‡ºä¸€ä¸ªç»“è®ºï¼š
   - æ€»åˆ† â‰¥ 4.0ï¼šç»“è®ºå†™"å€¼å¾—é˜…è¯»"
   - 3.0â€“3.9ï¼šç»“è®ºå†™"ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»"
   - ï¼œ 3.0ï¼šç»“è®ºå†™"ä¸å¤ªå€¼å¾—é˜…è¯»"
4. ç”¨ 2â€“4 å¥è¯ç®€è¦è¯´æ˜Žç†ç”±ï¼Œå¹¶æŒ‡å‡º 1â€“2 ä¸ªä¸»è¦ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚

è¾“å‡ºå¿…é¡»ä½¿ç”¨å¦‚ä¸‹ JSON æ ¼å¼ï¼Œä¸è¦æ·»åŠ å¤šä½™è¯´æ˜Žï¼š

{{
  "relevance_score": åˆ†æ•°,
  "informativeness_accuracy_score": åˆ†æ•°,
  "depth_opinion_score": åˆ†æ•°,
  "readability_score": åˆ†æ•°,
  "non_redundancy_score": åˆ†æ•°,
  "overall_score": æ€»åˆ†,
  "verdict": "å€¼å¾—é˜…è¯»/ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»/ä¸å¤ªå€¼å¾—é˜…è¯»",
  "comment": "2-4 å¥è¯è¯´æ˜Žç†ç”±ï¼ŒåŒ…å«ä¸»è¦ä¼˜ç‚¹å’Œç¼ºç‚¹"
}}

ä¸‹é¢æ˜¯æ–‡ç« å†…å®¹ï¼š

æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary[:200] if summary else 'æ— '}
æ­£æ–‡ï¼š{content[:3000]}
"""


def parse_score_response(response_text: str) -> Dict[str, Any]:
    """
    è§£æž LLM è¯„åˆ†å“åº”
    
    Args:
        response_text: LLM å“åº”æ–‡æœ¬
    
    Returns:
        è§£æžåŽçš„è¯„åˆ†ç»“æžœ
    """
    try:
        # å°è¯•æå– JSON
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = [
                "relevance_score", "informativeness_accuracy_score",
                "depth_opinion_score", "readability_score",
                "non_redundancy_score", "overall_score", "verdict", "comment"
            ]
            
            if all(field in result for field in required_fields):
                return result
        
        # è§£æžå¤±è´¥ï¼Œè¿”å›žé»˜è®¤å€¼
        return {
            "relevance_score": 3,
            "informativeness_accuracy_score": 3,
            "depth_opinion_score": 3,
            "readability_score": 3,
            "non_redundancy_score": 3,
            "overall_score": 3.0,
            "verdict": "ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»",
            "comment": f"æ— æ³•è§£æžè¯„åˆ†å“åº”: {response_text[:100]}"
        }
    except Exception as e:
        logger.error(f"è§£æžè¯„åˆ†å“åº”å¤±è´¥: {e}")
        return {
            "relevance_score": 0,
            "informativeness_accuracy_score": 0,
            "depth_opinion_score": 0,
            "readability_score": 0,
            "non_redundancy_score": 0,
            "overall_score": 0.0,
            "verdict": "ä¸å¤ªå€¼å¾—é˜…è¯»",
            "comment": f"è¯„åˆ†è§£æžé”™è¯¯: {str(e)}"
        }


def score_article(title: str, summary: str, content: str) -> Dict[str, Any]:
    """
    å¯¹æ–‡ç« è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        summary: æ–‡ç« æ‘˜è¦
        content: æ–‡ç« å†…å®¹
    
    Returns:
        è¯„åˆ†ç»“æžœå­—å…¸
    """
    try:
        # ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„ analysis_profile
        analysis_profile = PROJ_CONFIG.get("analysis_profile")
        
        client = OpenAI(
            api_key=get_config("OPENAI_API_KEY", profile=analysis_profile),
            base_url=get_config("OPENAI_BASE_URL", profile=analysis_profile)
        )
        
        prompt = build_scoring_prompt(title, summary, content)
        log_debug("Scoring Prompt", prompt)
        
        response = client.chat.completions.create(
            model=get_config("OPENAI_MODEL", "gpt-4o-mini", profile=analysis_profile),
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # é™ä½Žæ¸©åº¦ä»¥èŽ·å¾—æ›´ä¸€è‡´çš„è¯„åˆ†
            max_tokens=1024
        )
        
        response_text = response.choices[0].message.content
        log_debug("Scoring Response", response_text)
        
        if not response_text:
            return {
                "relevance_score": 0,
                "informativeness_accuracy_score": 0,
                "depth_opinion_score": 0,
                "readability_score": 0,
                "non_redundancy_score": 0,
                "overall_score": 0.0,
                "verdict": "ä¸å¤ªå€¼å¾—é˜…è¯»",
                "comment": "æ¨¡åž‹æœªè¿”å›žå†…å®¹"
            }
        
        return parse_score_response(response_text)
        
    except Exception as e:
        logger.error(f"è¯„åˆ†å¤±è´¥: {e}")
        import traceback
        import sys
        traceback.print_exc(file=sys.stderr)
        return {
            "relevance_score": 0,
            "informativeness_accuracy_score": 0,
            "depth_opinion_score": 0,
            "readability_score": 0,
            "non_redundancy_score": 0,
            "overall_score": 0.0,
            "verdict": "ä¸å¤ªå€¼å¾—é˜…è¯»",
            "comment": f"è¯„åˆ†å¤±è´¥: {str(e)}"
        }


def format_score_result(score_result: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–è¯„åˆ†ç»“æžœä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    Args:
        score_result: è¯„åˆ†ç»“æžœå­—å…¸
    
    Returns:
        æ ¼å¼åŒ–åŽçš„å­—ç¬¦ä¸²
    """
    verdict = score_result.get("verdict", "æœªçŸ¥")
    overall = score_result.get("overall_score", 0.0)
    
    # æ·»åŠ  emoji
    emoji = "ðŸ˜"
    if overall >= 4.0:
        emoji = "ðŸ”¥"
    elif overall < 3.0:
        emoji = "ðŸ‘Ž"
    
    return f"{emoji} {verdict} ({overall}/5.0)"
