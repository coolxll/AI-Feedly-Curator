"""
LLM åˆ†ææ¨¡å—
ä½¿ç”¨ OpenAI å…¼å®¹ API è¿›è¡Œæ–‡ç« åˆ†æå’Œæ‘˜è¦ç”Ÿæˆ
"""
import json
import re
import logging
import traceback
import sys

from openai import OpenAI

from .config import PROJ_CONFIG, get_config, log_debug

logger = logging.getLogger(__name__)


def analyze_article_with_llm(title: str, summary: str, content: str) -> dict:
    """
    ä½¿ç”¨ OpenAI å…¼å®¹ API åˆ†ææ–‡ç« 
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        summary: æ–‡ç« æ‘˜è¦
        content: æ–‡ç« å†…å®¹
    
    Returns:
        åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«è¯¦ç»†è¯„åˆ†å’Œç®€çŸ­æ‘˜è¦
    """
    from .scoring import score_article, format_score_result
    
    try:
        # ä½¿ç”¨æ–°çš„è¯„åˆ†ç³»ç»Ÿ
        score_result = score_article(title, summary, content)
        
        # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
        return {
            "score": score_result.get("overall_score", 0.0),
            "verdict": score_result.get("verdict", "æœªçŸ¥"),
            "summary": score_result.get("comment", ""),
            "reason": format_score_result(score_result),
            "detailed_scores": {
                "relevance": score_result.get("relevance_score", 0),
                "informativeness": score_result.get("informativeness_accuracy_score", 0),
                "depth": score_result.get("depth_opinion_score", 0),
                "readability": score_result.get("readability_score", 0),
                "originality": score_result.get("non_redundancy_score", 0)
            }
        }
    except Exception as e:
        logger.error(f"æ–‡ç« åˆ†æå¤±è´¥: {e}")
        import traceback
        import sys
        traceback.print_exc(file=sys.stderr)
        return {
            "score": 0.0,
            "verdict": "ä¸å¤ªå€¼å¾—é˜…è¯»",
            "summary": f"åˆ†æå¤±è´¥: {str(e)}",
            "reason": "APIè°ƒç”¨é”™è¯¯",
            "detailed_scores": {
                "relevance": 0,
                "informativeness": 0,
                "depth": 0,
                "readability": 0,
                "originality": 0
            }
        }



def generate_overall_summary(analyzed_articles: list) -> str:
    """
    ç”Ÿæˆæ€»ä½“æ‘˜è¦
    
    Args:
        analyzed_articles: å·²åˆ†æçš„æ–‡ç« åˆ—è¡¨
    
    Returns:
        Markdown æ ¼å¼çš„æ€»ä½“æ‘˜è¦
    """
    try:
        # ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„ summary_profile
        summary_profile = PROJ_CONFIG.get("summary_profile")
        
        # ä¼˜å…ˆä½¿ç”¨ Summary ä¸“ç”¨çš„é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ fallback åˆ°é€šç”¨é…ç½®
        api_key = get_config("OPENAI_SUMMARY_API_KEY", profile=summary_profile) or get_config("OPENAI_API_KEY", profile=summary_profile)
        base_url = get_config("OPENAI_SUMMARY_BASE_URL", profile=summary_profile) or get_config("OPENAI_BASE_URL", "https://api.openai.com/v1", profile=summary_profile)
        
        client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        
        # å‡†å¤‡æ–‡ç« åˆ—è¡¨
        articles_info = []
        for article in analyzed_articles:
            analysis = article["analysis"]
            articles_info.append({
                "title": article["title"],
                "link": article.get("link", ""),
                "score": analysis.get("score", 0.0),
                "verdict": analysis.get("verdict", "æœªçŸ¥"),
                "summary": analysis.get("summary", ""),
                "detailed_scores": analysis.get("detailed_scores", {})
            })
        
        prompt = f"""åŸºäºä»¥ä¸‹å·²è¯„åˆ†å’Œæ‘˜è¦çš„æ–‡ç« åˆ—è¡¨,ç”Ÿæˆä¸€ä»½æ€»ä½“æ‘˜è¦æŠ¥å‘Šï¼š

æ–‡ç« åˆ—è¡¨ï¼š
{json.dumps(articles_info, ensure_ascii=False, indent=2)}

è¯„åˆ†è¯´æ˜ï¼š
- è¯„åˆ†èŒƒå›´ï¼š0-5.0 åˆ†
- åˆ†ç±»æ ‡å‡†ï¼šâ‰¥4.0 å€¼å¾—é˜…è¯»ï¼Œ3.0-3.9 ä¸€èˆ¬ï¼Œ<3.0 ä¸æ¨è

è¯·æä¾›ï¼š
1. æ•´ä½“è¶‹åŠ¿åˆ†æ
2. é«˜åˆ†æ–‡ç« ï¼ˆâ‰¥4.0åˆ†ï¼‰çš„å…±åŒç‰¹ç‚¹å’Œæ¨èç†ç”±
3. ä¸»è¦è¯é¢˜åˆ†ç±»
4. æ¨èé˜…è¯»ä¼˜å…ˆçº§ï¼ˆæŒ‰è¯„åˆ†æ’åºï¼‰

æ³¨æ„ï¼š
- è¯·ç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„å†…å®¹ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹
- æåˆ°æ–‡ç« æ—¶ï¼Œè¯·ä½¿ç”¨ Markdown é“¾æ¥æ ¼å¼ï¼š[æ–‡ç« æ ‡é¢˜](link)
- åœ¨æ¨èæ–‡ç« æ—¶ï¼Œè¯·æ ‡æ³¨è¯„åˆ†å’Œç»“è®ºï¼ˆå¦‚ï¼šğŸ”¥ å€¼å¾—é˜…è¯» 4.2/5.0ï¼‰
"""
        
        # è·å–é…ç½®å‚æ•° - ä¼˜å…ˆä½¿ç”¨ SUMMARY_MODELï¼Œå¦åˆ™ä½¿ç”¨ profile çš„é€šç”¨ MODEL
        model = (
            get_config("OPENAI_SUMMARY_MODEL", profile=summary_profile) or 
            get_config("OPENAI_MODEL", "gpt-4o-mini", profile=summary_profile)
        )
        temperature = 0.7
        extra_body = {"enable_thinking": True}
        
        # æ‰“å°è¯·æ±‚ä¿¡æ¯ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼Œä¾¿äºè°ƒè¯•ï¼‰
        print(f"\n{'='*60}")
        print("OpenAI API è¯·æ±‚ä¿¡æ¯:")
        print(f"{'='*60}")
        print(f"Base URL: {base_url}")
        print(f"Model: {model}")
        print(f"Temperature: {temperature}")
        print(f"Extra Body: {json.dumps(extra_body, ensure_ascii=False)}")
        print(f"API Key: {'*' * 20}{api_key[-8:] if api_key else 'NOT SET'}")
        print(f"\næç¤ºè¯ (Prompt):\n{'-'*60}\n{prompt}\n{'-'*60}\n")
        
        # å‘é€è¯·æ±‚
        print("æ­£åœ¨å‘é€è¯·æ±‚åˆ° OpenAI API...")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            extra_body=extra_body,
            temperature=temperature
        )
        
        # æ‰“å°å“åº”ä¿¡æ¯
        print(f"\n{'='*60}")
        print("OpenAI API å“åº”ä¿¡æ¯:")
        print(f"{'='*60}")
        print(f"Response ID: {response.id if hasattr(response, 'id') else 'N/A'}")
        print(f"Model: {response.model if hasattr(response, 'model') else 'N/A'}")
        print(f"Created: {response.created if hasattr(response, 'created') else 'N/A'}")
        
        # æ‰“å°ä½¿ç”¨ç»Ÿè®¡
        if hasattr(response, 'usage') and response.usage:
            print(f"\nToken ä½¿ç”¨ç»Ÿè®¡:")
            print(f"  - Prompt Tokens: {response.usage.prompt_tokens if hasattr(response.usage, 'prompt_tokens') else 'N/A'}")
            print(f"  - Completion Tokens: {response.usage.completion_tokens if hasattr(response.usage, 'completion_tokens') else 'N/A'}")
            print(f"  - Total Tokens: {response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else 'N/A'}")
        
        # æ‰“å°é€‰æ‹©ä¿¡æ¯
        if response.choices and len(response.choices) > 0:
            choice = response.choices[0]
            print(f"\nå“åº”è¯¦æƒ…:")
            print(f"  - Finish Reason: {choice.finish_reason if hasattr(choice, 'finish_reason') else 'N/A'}")
            print(f"  - Index: {choice.index if hasattr(choice, 'index') else 'N/A'}")
            
            # æ‰“å°å“åº”å†…å®¹
            content = choice.message.content if hasattr(choice.message, 'content') else None
            if content:
                print(f"\nå“åº”å†…å®¹ (å‰500å­—ç¬¦):\n{'-'*60}")
                print(content[:500])
                if len(content) > 500:
                    print(f"... (æ€»å…± {len(content)} å­—ç¬¦)")
                print(f"{'-'*60}")
            else:
                print("\nâš ï¸ è­¦å‘Š: å“åº”å†…å®¹ä¸ºç©º!")
        else:
            print("\nâš ï¸ è­¦å‘Š: æ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹© (choices)!")
        
        print(f"{'='*60}\n")
        
        content = response.choices[0].message.content
        if not content:
            return "ç”Ÿæˆæ€»ç»“å¤±è´¥: æ¨¡å‹æœªè¿”å›å†…å®¹"
        return content
    except Exception as e:
        print(f"\n{'='*60}")
        print("âŒ OpenAI API è°ƒç”¨å¤±è´¥:")
        print(f"{'='*60}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"\nå®Œæ•´å †æ ˆè·Ÿè¸ª:")
        traceback.print_exc(file=sys.stderr)
        print(f"{'='*60}\n")
        return f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}"
