"""
LLM åˆ†ææ¨¡å—
ä½¿ç”¨ OpenAI å…¼å®¹ API è¿›è¡Œæ–‡ç« åˆ†æå’Œæ‘˜è¦ç”Ÿæˆ
"""
import json
import logging
import traceback
import sys

from openai import OpenAI

from .config import PROJ_CONFIG, get_config

logger = logging.getLogger(__name__)


def analyze_article_with_llm(title: str, summary: str, content: str) -> dict:
    """
    ä½¿ç”¨ OpenAI å…¼å®¹ API åˆ†ææ–‡ç« 

    Args:
        title: æ–‡ç« æ ‡é¢˜
        summary: æ–‡ç« æ‘˜è¦
        content: æ–‡ç« å†…å®¹

    Returns:
        åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«è¯¦ç»†è¯„åˆ†å’Œç®€çŸ­æ‘˜è¦
    """
    from .scoring import score_article, format_score_result

    try:
        # ä½¿ç”¨æ–°çš„è¯„åˆ†ç³»ç»Ÿ
        score_result = score_article(title, summary, content)

        # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
        return {
            "score": score_result.get("overall_score", 0.0),
            "verdict": score_result.get("verdict", "æœªçŸ¥"),
            "summary": score_result.get("comment", ""),
            "reason": format_score_result(score_result),
            "model": score_result.get("model", "unknown"),
            "detailed_scores": {
                "relevance": score_result.get("relevance_score", 0),
                "informativeness": score_result.get("informativeness_accuracy_score", 0),
                "depth": score_result.get("depth_opinion_score", 0),
                "readability": score_result.get("readability_score", 0),
                "originality": score_result.get("non_redundancy_score", 0),
                "red_flags": score_result.get("red_flags", []),
                "article_type": score_result.get("article_type", "default")
            }
        }
    except Exception as e:
        logger.error(f"æ–‡ç« åˆ†æå¤±è´¥: {e}")
        import traceback
        import sys
        traceback.print_exc(file=sys.stderr)
        return {
            "score": 0.0,
            "verdict": "ä¸å¤ªå€¼å¾—é˜…è¯»",
            "summary": f"åˆ†æå¤±è´¥: {str(e)}",
            "reason": "APIè°ƒç”¨é”™è¯¯",
            "detailed_scores": {
                "relevance": 0,
                "informativeness": 0,
                "depth": 0,
                "readability": 0,
                "originality": 0
            }
        }



def analyze_articles_with_llm_batch(articles: list[dict]) -> list[dict]:
    """
    ä½¿ç”¨ OpenAI å…¼å®¹ API æ‰¹é‡åˆ†ææ–‡ç« 

    Args:
        articles: [{title, summary, content}, ...]
    Returns:
        åˆ†æç»“æœåˆ—è¡¨ï¼Œä¸è¾“å…¥é¡ºåºä¸€è‡´
    """
    from .scoring import score_articles_batch, format_score_result

    try:
        score_results = score_articles_batch(articles)
        if not score_results or len(score_results) != len(articles):
            raise ValueError("batch scoring failed or size mismatch")

        analyzed = []
        for score_result in score_results:
            analyzed.append({
                "score": score_result.get("overall_score", 0.0),
                "verdict": score_result.get("verdict", "æœªçŸ¥"),
                "summary": score_result.get("comment", ""),
                "reason": format_score_result(score_result),
                "model": score_result.get("model", "unknown"),
                "detailed_scores": {
                    "relevance": score_result.get("relevance_score", 0),
                    "informativeness": score_result.get("informativeness_accuracy_score", 0),
                    "depth": score_result.get("depth_opinion_score", 0),
                    "readability": score_result.get("readability_score", 0),
                    "originality": score_result.get("non_redundancy_score", 0),
                    "red_flags": score_result.get("red_flags", []),
                    "article_type": score_result.get("article_type", "default")
                }
            })

        return analyzed
    except Exception as e:
        logger.warning(f"æ‰¹é‡è¯„åˆ†å¤±è´¥ï¼Œå›é€€ä¸ºå•ç¯‡è¯„åˆ†: {e}")
        fallback = []
        for article in articles:
            fallback.append(
                analyze_article_with_llm(
                    article.get("title", ""),
                    article.get("summary", ""),
                    article.get("content", "")
                )
            )
        return fallback


def generate_overall_summary(analyzed_articles: list) -> str:
    """
    ç”Ÿæˆæ€»ä½“æ‘˜è¦

    Args:
        analyzed_articles: å·²åˆ†æçš„æ–‡ç« åˆ—è¡¨

    Returns:
        Markdown æ ¼å¼çš„æ€»ä½“æ‘˜è¦
    """
    try:
        # ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„ summary_profile
        summary_profile = PROJ_CONFIG.get("summary_profile")

        # ä¼˜å…ˆä½¿ç”¨ Summary ä¸“ç”¨çš„é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ fallback åˆ°é€šç”¨é…ç½®
        api_key = get_config("OPENAI_SUMMARY_API_KEY", profile=summary_profile) or get_config("OPENAI_API_KEY", profile=summary_profile)
        base_url = get_config("OPENAI_SUMMARY_BASE_URL", profile=summary_profile) or get_config("OPENAI_BASE_URL", "https://api.openai.com/v1", profile=summary_profile)

        client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )

        # å‡†å¤‡æ–‡ç« åˆ—è¡¨ (è¿‡æ»¤æ‰ä½è´¨é‡æ–‡ç« ä»¥èŠ‚çœ Token)
        articles_info = []
        skipped_count = 0

        for article in analyzed_articles:
            analysis = article["analysis"]
            score = analysis.get("score", 0.0)
            red_flags = analysis.get("detailed_scores", {}).get("red_flags", [])

            # è¿‡æ»¤é€»è¾‘ï¼š
            # 1. åˆ†æ•° < 3.0 (ä¸æ¨è)
            # 2. åŒ…å« Red Flags (è½¯æ–‡/æ ‡é¢˜å…šç­‰)
            if score < 3.0 or red_flags:
                skipped_count += 1
                continue

            articles_info.append({
                "title": article["title"],
                "link": article.get("link", ""),
                "score": score,
                "verdict": analysis.get("verdict", "æœªçŸ¥"),
                "summary": analysis.get("summary", ""),
                "detailed_scores": analysis.get("detailed_scores", {})
            })

        if not articles_info:
            return "æ²¡æœ‰å€¼å¾—æ€»ç»“çš„é«˜è´¨é‡æ–‡ç« ã€‚"

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹ç¼–è¾‘ï¼Œè¯·åŸºäºä»¥ä¸‹å·²ç­›é€‰çš„é«˜è´¨é‡æ–‡ç« åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤æ‰ä½åˆ†å’Œåƒåœ¾å†…å®¹ï¼‰ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„é˜…è¯»æ¨èæŠ¥å‘Šã€‚

æ–‡ç« åˆ—è¡¨ï¼ˆå…± {len(articles_info)} ç¯‡ï¼Œå·²è·³è¿‡ {skipped_count} ç¯‡ä½è´¨é‡æ–‡ç« ï¼‰ï¼š
{json.dumps(articles_info, ensure_ascii=False, indent=2)}

è¯„åˆ†è¯´æ˜ï¼š
- è¯„åˆ†èŒƒå›´ï¼š0-5.0 åˆ†
- åˆ†ç±»æ ‡å‡†ï¼š
  * â‰¥4.0 åˆ†ï¼šğŸ”¥ å€¼å¾—é˜…è¯»
  * 3.0-3.9 åˆ†ï¼šğŸ˜ ä¸€èˆ¬ï¼Œå¯é€‰é˜…è¯»
  * <3.0 åˆ†ï¼šğŸ‘ ä¸å¤ªå€¼å¾—é˜…è¯»
- è¯„åˆ†ç»´åº¦ï¼šç›¸å…³æ€§ã€ä¿¡æ¯é‡ã€æ·±åº¦ã€å¯è¯»æ€§ã€åŸåˆ›æ€§ï¼ˆå„ 1-5 åˆ†ï¼‰

è¯·æŒ‰ä»¥ä¸‹ç»“æ„ç”ŸæˆæŠ¥å‘Šï¼š

## ğŸ“Š æ•´ä½“æ¦‚å†µ
- æ–‡ç« æ€»æ•°å’Œè¯„åˆ†åˆ†å¸ƒ
- å¹³å‡åˆ†å’Œæ•´ä½“è´¨é‡è¯„ä»·
- ä¸»è¦è¯é¢˜é¢†åŸŸ

## ğŸ”¥ å¼ºçƒˆæ¨èï¼ˆâ‰¥4.0åˆ†ï¼‰
å¯¹äºæ¯ç¯‡é«˜åˆ†æ–‡ç« ï¼Œè¯·æä¾›ï¼š
1. æ ‡é¢˜ï¼ˆå¸¦é“¾æ¥ï¼‰å’Œè¯„åˆ†
2. æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆå€¼å¾—è¯»ï¼Ÿæœ‰ä»€ä¹ˆäº®ç‚¹ï¼Ÿï¼‰
3. æ ¸å¿ƒè§‚ç‚¹æˆ–ä»·å€¼ï¼ˆ1-2å¥è¯ï¼‰

## ğŸ˜ å¯é€‰é˜…è¯»ï¼ˆ3.0-3.9åˆ†ï¼‰
åˆ—å‡ºè¿™äº›æ–‡ç« ï¼ˆæ ‡é¢˜å¸¦é“¾æ¥ï¼‰ï¼Œå¹¶ç®€è¦è¯´æ˜é€‚åˆä»€ä¹ˆæƒ…å†µä¸‹é˜…è¯»ã€‚
æ ¼å¼ç¤ºä¾‹ï¼š
- [æ–‡ç« æ ‡é¢˜](é“¾æ¥) **è¯„åˆ†: 3.5/5.0** - é€‚ç”¨åœºæ™¯è¯´æ˜

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ
- é«˜åˆ†æ–‡ç« çš„å…±åŒç‰¹ç‚¹
- ä¸»è¦è¯é¢˜åˆ†ç±»
- å€¼å¾—å…³æ³¨çš„æ–¹å‘

## ğŸ’¡ é˜…è¯»å»ºè®®
æ ¹æ®è¯„åˆ†å’Œå†…å®¹ï¼Œç»™å‡ºä¼˜å…ˆçº§å»ºè®®

æ³¨æ„äº‹é¡¹ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹
- æ–‡ç« æ ‡é¢˜ä½¿ç”¨é“¾æ¥æ ¼å¼ï¼š[æ ‡é¢˜](link)
- åœ¨æ¯ç¯‡æ¨èæ–‡ç« åæ ‡æ³¨è¯„åˆ†ï¼Œå¦‚ï¼š**è¯„åˆ†: 4.5/5.0** ğŸ”¥
- å……åˆ†åˆ©ç”¨ summary å­—æ®µä¸­çš„æ–‡ç« è¯„ä»·ä¿¡æ¯
- æ¨èç†ç”±è¦å…·ä½“ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
"""

        # è·å–é…ç½®å‚æ•° - ä¼˜å…ˆä½¿ç”¨ SUMMARY_MODELï¼Œå¦åˆ™ä½¿ç”¨ profile çš„é€šç”¨ MODEL
        model = (
            get_config("OPENAI_MODEL", "gpt-4o-mini", profile=summary_profile)
        )
        temperature = 0.7
        extra_body = {"enable_thinking": True}

        # æ‰“å°è¯·æ±‚ä¿¡æ¯ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼Œä¾¿äºè°ƒè¯•ï¼‰
        print(f"\n{'='*60}")
        print("OpenAI API è¯·æ±‚ä¿¡æ¯:")
        print(f"{'='*60}")
        print(f"Base URL: {base_url}")
        print(f"Model: {model}")
        print(f"Temperature: {temperature}")
        print(f"Extra Body: {json.dumps(extra_body, ensure_ascii=False)}")
        print(f"API Key: {'*' * 20}{api_key[-8:] if api_key else 'NOT SET'}")
        print(f"\næç¤ºè¯ (Prompt):\n{'-'*60}\n{prompt}\n{'-'*60}\n")

        # å‘é€è¯·æ±‚
        print("æ­£åœ¨å‘é€è¯·æ±‚åˆ° OpenAI API...")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            extra_body=extra_body,
            temperature=temperature
        )

        # æ‰“å°å“åº”ä¿¡æ¯
        print(f"\n{'='*60}")
        print("OpenAI API å“åº”ä¿¡æ¯:")
        print(f"{'='*60}")
        print(f"Response ID: {response.id if hasattr(response, 'id') else 'N/A'}")
        print(f"Model: {response.model if hasattr(response, 'model') else 'N/A'}")
        print(f"Created: {response.created if hasattr(response, 'created') else 'N/A'}")

        # æ‰“å°ä½¿ç”¨ç»Ÿè®¡
        if hasattr(response, 'usage') and response.usage:
            print(f"\nToken ä½¿ç”¨ç»Ÿè®¡:")
            print(f"  - Prompt Tokens: {response.usage.prompt_tokens if hasattr(response.usage, 'prompt_tokens') else 'N/A'}")
            print(f"  - Completion Tokens: {response.usage.completion_tokens if hasattr(response.usage, 'completion_tokens') else 'N/A'}")
            print(f"  - Total Tokens: {response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else 'N/A'}")

        # æ‰“å°é€‰æ‹©ä¿¡æ¯
        if response.choices and len(response.choices) > 0:
            choice = response.choices[0]
            print("\nå“åº”è¯¦æƒ…:")
            print(f"  - Finish Reason: {choice.finish_reason if hasattr(choice, 'finish_reason') else 'N/A'}")
            print(f"  - Index: {choice.index if hasattr(choice, 'index') else 'N/A'}")

            # æ‰“å°å“åº”å†…å®¹
            content = choice.message.content if hasattr(choice.message, 'content') else None
            if content:
                print(f"\nå“åº”å†…å®¹ (å‰500å­—ç¬¦):\n{'-'*60}")
                print(content[:500])
                if len(content) > 500:
                    print(f"... (æ€»å…± {len(content)} å­—ç¬¦)")
                print(f"{'-'*60}")
            else:
                print("\nâš ï¸ è­¦å‘Š: å“åº”å†…å®¹ä¸ºç©º!")
        else:
            print("\nâš ï¸ è­¦å‘Š: æ²¡æœ‰è¿”å›ä»»ä½•é€‰æ‹© (choices)!")

        print(f"{'='*60}\n")

        content = response.choices[0].message.content
        if not content:
            return "ç”Ÿæˆæ€»ç»“å¤±è´¥: æ¨¡å‹æœªè¿”å›å†…å®¹"
        return content
    except Exception as e:
        print(f"\n{'='*60}")
        print("âŒ OpenAI API è°ƒç”¨å¤±è´¥:")
        print(f"{'='*60}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print("\nå®Œæ•´å †æ ˆè·Ÿè¸ª:")
        traceback.print_exc(file=sys.stderr)
        print(f"{'='*60}\n")
        return f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}"
