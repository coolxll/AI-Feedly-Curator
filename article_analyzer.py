#!/usr/bin/env python3
"""
RSS æ–‡ç« åˆ†æå™¨
ä½¿ç”¨ AI åˆ†æ RSS è®¢é˜…æ–‡ç« ï¼Œç”Ÿæˆè¯„åˆ†å’Œæ‘˜è¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python article_analyzer.py [--refresh] [--limit N] [--mark-read] [--debug]

é…ç½®ï¼š
    ä¿®æ”¹ src/config.py ä¸­çš„ PROJ_CONFIG æ¥è°ƒæ•´é»˜è®¤è®¾ç½®
    ä¿®æ”¹ .env æ–‡ä»¶æ¥é…ç½® API å¯†é’¥å’Œ Profile
"""
import os
import argparse
import logging

# å¯¼å…¥æ¨¡å—
from rss_analyzer.config import PROJ_CONFIG, setup_logging
from rss_analyzer.feedly_client import feedly_fetch_unread, feedly_mark_read
from rss_analyzer.article_fetcher import fetch_article_content
from rss_analyzer.llm_analyzer import analyze_article_with_llm, generate_overall_summary
from rss_analyzer.utils import load_articles, save_articles

logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Article Analyzer")
    
    parser.add_argument("--input", default=PROJ_CONFIG["input_file"], 
                        help=f"Input JSON file (default: {PROJ_CONFIG['input_file']})")
    parser.add_argument("--limit", type=int, default=PROJ_CONFIG["limit"], 
                        help=f"Number of articles to process (default: {PROJ_CONFIG['limit']})")
    parser.add_argument("--mark-read", action="store_true", default=PROJ_CONFIG["mark_read"], 
                        help=f"Mark processed articles as read (default: {PROJ_CONFIG['mark_read']})")
    parser.add_argument("--debug", action="store_true", default=PROJ_CONFIG["debug"], 
                        help=f"Enable debug mode (default: {PROJ_CONFIG['debug']})")
    parser.add_argument("--refresh", action="store_true", default=PROJ_CONFIG["refresh"], 
                        help=f"Refresh from Feedly before processing (default: {PROJ_CONFIG['refresh']})")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    debug_mode = args.debug or os.getenv("DEBUG", "").lower() in ("true", "1", "yes")
    setup_logging(debug_mode)
    
    if debug_mode:
        logger.info("Debugæ¨¡å¼å·²å¯ç”¨")

    # åˆ·æ–°unread_news.json
    if args.refresh:
        logger.info("=" * 60)
        logger.info("ğŸ“¥ ä» Feedly åˆ·æ–°æ–‡ç« ")
        logger.info("=" * 60)
        logger.info(f"æ­£åœ¨è·å–æœ€æ–° {args.limit} ç¯‡æœªè¯»æ–‡ç« ...")
        articles = feedly_fetch_unread(limit=args.limit)
        if articles is None:
            logger.error("âŒ æ— æ³•ä» Feedly è·å–æ–‡ç« ï¼Œé€€å‡º")
            return
        
        output_file = "unread_news.json"
        save_articles(articles, output_file)
        logger.info(f"âœ“ å·²ä¿å­˜ {len(articles)} ç¯‡æœªè¯»æ–‡ç« åˆ° {output_file}")
        logger.info("")
        
        if args.input == PROJ_CONFIG["input_file"]:
            args.input = output_file
    else:
        logger.info("=" * 60)
        logger.info("ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ–‡ç« æ•°æ®ï¼ˆæœªåˆ·æ–°ï¼‰")
        logger.info("=" * 60)
        logger.info(f"æç¤º: ä½¿ç”¨ --refresh å‚æ•°å¯ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        logger.info("")

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_file = args.input
    if not os.path.exists(input_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info("æç¤º: ä½¿ç”¨ --refresh å‚æ•°ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        return

    # åŠ è½½æ–‡ç« åˆ—è¡¨
    articles = load_articles(input_file)
    logger.info(f"ğŸ“– ä» {input_file} åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
    logger.info(f"ğŸ¯ å°†å¤„ç†å‰ {min(args.limit, len(articles))} ç¯‡æ–‡ç« ")
    logger.info("")
    
    analyzed_articles = []
    processed_ids = []
    
    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    for idx, article in enumerate(articles[:args.limit], 1):
        logger.info(f"å¤„ç†ç¬¬ {idx}/{min(args.limit, len(articles))} ç¯‡: {article['title']}")
        
        summary = article.get('summary', '')
        if summary and len(summary) > 500:
            logger.info(f"  âœ“ æ‘˜è¦è¾ƒé•¿ ({len(summary)} å­—ç¬¦)ï¼Œè·³è¿‡ç½‘é¡µæŠ“å–")
            content = summary
        else:
            logger.info(f"  â†’ å¼€å§‹æŠ“å–ç½‘é¡µå†…å®¹...")
            content = fetch_article_content(article['link'])
            logger.info(f"  âœ“ æŠ“å–å®Œæˆ: {len(content)} å­—ç¬¦")
        
        analysis = analyze_article_with_llm(article['title'], summary, content)
        logger.info(f"  âœ“ è¯„åˆ†: {analysis['score']:.1f}/5.0 - {analysis.get('verdict', 'æœªçŸ¥')}")
        logger.info(f"  âœ“ è¯„ä»·: {analysis.get('reason', '')}")
        if 'detailed_scores' in analysis:
            scores = analysis['detailed_scores']
            logger.info(f"     ç›¸å…³æ€§:{scores['relevance']} ä¿¡æ¯é‡:{scores['informativeness']} "
                       f"æ·±åº¦:{scores['depth']} å¯è¯»æ€§:{scores['readability']} åŸåˆ›æ€§:{scores['originality']}")

        
        analyzed_articles.append({**article, "analysis": analysis})

        if article.get('id'):
            processed_ids.append(article['id'])
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ŒæŒ‰æœˆä»½ç»„ç»‡
    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜åˆ†æç»“æœåˆ°å½’æ¡£ç›®å½•
    analyzed_file = os.path.join(output_dir, f"analyzed_articles_{timestamp}.json")
    save_articles(analyzed_articles, analyzed_file)
    
    # åŒæ—¶ä¿å­˜åˆ°æ ¹ç›®å½•ï¼ˆä¸ºäº†å…¼å®¹æ€§å’Œæ–¹ä¾¿è®¿é—®ï¼‰
    save_articles(analyzed_articles, 'analyzed_articles.json')
    
    logger.info(f"\nåˆ†æç»“æœå·²ä¿å­˜åˆ°:")
    logger.info(f"  - {analyzed_file}")
    logger.info(f"  - analyzed_articles.json (æœ€æ–°ç‰ˆæœ¬)")
    
    # æ ‡è®°å·²è¯»
    if args.mark_read and processed_ids:
        logger.info(f"\næ­£åœ¨æ ‡è®° {len(processed_ids)} ç¯‡æ–‡ç« ä¸ºå·²è¯»...")
        feedly_mark_read(processed_ids)
    
    # ç”Ÿæˆæ€»ä½“æ‘˜è¦
    logger.info("\nç”Ÿæˆæ€»ä½“æ‘˜è¦...")
    overall_summary = generate_overall_summary(analyzed_articles)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ŒæŒ‰æœˆä»½ç»„ç»‡
    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    summary_file = os.path.join(output_dir, f"summary_{timestamp}.md")
    
    # åŒæ—¶ä¿å­˜åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆåœ¨æ ¹ output ç›®å½•ï¼‰
    latest_file = os.path.join("output", "summary_latest.md")
    
    # ä¿å­˜æ‘˜è¦ï¼ˆä¸ analyzed_articles åœ¨åŒä¸€ç›®å½•ï¼‰
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    with open(latest_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    
    logger.info(f"æ€»ä½“æ‘˜è¦å·²ä¿å­˜åˆ°:")
    logger.info(f"  - {summary_file}")
    logger.info(f"  - {latest_file}")
    logger.info(f"\nå½’æ¡£æ–‡ä»¶:")
    logger.info(f"  - {analyzed_file}")
    logger.info(f"  - {summary_file}")
    
    logger.info("\n" + "="*50)
    logger.info("æ€»ä½“æ‘˜è¦:")
    logger.info("="*50)
    logger.info(f"\n{overall_summary}")


if __name__ == "__main__":
    main()
