#!/usr/bin/env python3
"""
RSS æ–‡ç« åˆ†æå™¨
ä½¿ç”¨ AI åˆ†æ RSS è®¢é˜…æ–‡ç« ï¼Œç”Ÿæˆè¯„åˆ†å’Œæ‘˜è¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python article_analyzer.py [--refresh] [--limit N] [--mark-read] [--debug]

é…ç½®ï¼š
    ä¿®æ”¹ src/config.py ä¸­çš„ PROJ_CONFIG æ¥è°ƒæ•´é»˜è®¤è®¾ç½®
    ä¿®æ”¹ .env æ–‡ä»¶æ¥é…ç½® API å¯†é’¥å’Œ Profile
"""
import os
import argparse
import logging

# å¯¼å…¥æ¨¡å—
from rss_analyzer.config import PROJ_CONFIG, setup_logging
from rss_analyzer.feedly_client import feedly_fetch_unread, feedly_mark_read
from rss_analyzer.article_fetcher import fetch_article_content
from rss_analyzer.llm_analyzer import analyze_article_with_llm, generate_overall_summary
from rss_analyzer.utils import load_articles, save_articles

logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Article Analyzer")
    
    parser.add_argument("--input", default=PROJ_CONFIG["input_file"], 
                        help=f"Input JSON file (default: {PROJ_CONFIG['input_file']})")
    parser.add_argument("--limit", type=int, default=PROJ_CONFIG["limit"], 
                        help=f"Number of articles to process (default: {PROJ_CONFIG['limit']})")
    parser.add_argument("--mark-read", action="store_true", default=PROJ_CONFIG["mark_read"], 
                        help=f"Mark processed articles as read (default: {PROJ_CONFIG['mark_read']})")
    parser.add_argument("--debug", action="store_true", default=PROJ_CONFIG["debug"], 
                        help=f"Enable debug mode (default: {PROJ_CONFIG['debug']})")
    parser.add_argument("--refresh", action="store_true", default=PROJ_CONFIG["refresh"], 
                        help=f"Refresh from Feedly before processing (default: {PROJ_CONFIG['refresh']})")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    debug_mode = args.debug or os.getenv("DEBUG", "").lower() in ("true", "1", "yes")
    setup_logging(debug_mode)
    
    if debug_mode:
        logger.info("Debugæ¨¡å¼å·²å¯ç”¨")

    # åˆ·æ–°unread_news.json
    if args.refresh:
        logger.info("=" * 60)
        logger.info("ğŸ“¥ ä» Feedly åˆ·æ–°æ–‡ç« ")
        logger.info("=" * 60)
        logger.info(f"æ­£åœ¨è·å–æœ€æ–° {args.limit} ç¯‡æœªè¯»æ–‡ç« ...")
        articles = feedly_fetch_unread(limit=args.limit)
        if articles is None:
            logger.error("âŒ æ— æ³•ä» Feedly è·å–æ–‡ç« ï¼Œé€€å‡º")
            return
        
        output_file = "unread_news.json"
        save_articles(articles, output_file)
        logger.info(f"âœ“ å·²ä¿å­˜ {len(articles)} ç¯‡æœªè¯»æ–‡ç« åˆ° {output_file}")
        logger.info("")
        
        if args.input == PROJ_CONFIG["input_file"]:
            args.input = output_file
    else:
        logger.info("=" * 60)
        logger.info("ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ–‡ç« æ•°æ®ï¼ˆæœªåˆ·æ–°ï¼‰")
        logger.info("=" * 60)
        logger.info(f"æç¤º: ä½¿ç”¨ --refresh å‚æ•°å¯ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        logger.info("")

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_file = args.input
    if not os.path.exists(input_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info("æç¤º: ä½¿ç”¨ --refresh å‚æ•°ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        return

    # åŠ è½½æ–‡ç« åˆ—è¡¨
    articles = load_articles(input_file)
    logger.info(f"ğŸ“– ä» {input_file} åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
    logger.info(f"ğŸ¯ å°†å¤„ç†å‰ {min(args.limit, len(articles))} ç¯‡æ–‡ç« ")
    logger.info("")
    
    analyzed_articles = []
    processed_ids = []
    seen_titles = set()
    
    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    for idx, article in enumerate(articles[:args.limit], 1):
        logger.info(f"å¤„ç†ç¬¬ {idx}/{min(args.limit, len(articles))} ç¯‡: {article['title']}")
        
        # 1. å…³é”®è¯è¿‡æ»¤ (Pre-filtering)
        filter_keywords = PROJ_CONFIG.get("filter_keywords", [])
        if any(kw in article['title'] for kw in filter_keywords):
            logger.info(f"  ğŸš« æ ‡é¢˜åŒ…å«è¿‡æ»¤è¯ï¼Œè·³è¿‡")
            continue
        
        # 1.2 URLæ¨¡å¼è¿‡æ»¤ (Pre-filtering)
        filter_url_patterns = PROJ_CONFIG.get("filter_url_patterns", [])
        article_url = article.get('link', '') or article.get('originId', '')
        if any(pattern in article_url for pattern in filter_url_patterns):
            logger.info(f"  ğŸš« URLåŒ¹é…è¿‡æ»¤è§„åˆ™ ({article_url})ï¼Œè·³è¿‡")
            continue
            
        # 1.3 ç®€å•å»é‡ (Redundancy Filter)
        norm_title = "".join(filter(str.isalnum, article['title'].lower()))
        # æ£€æŸ¥æ˜¯å¦å¤ªçŸ­ï¼ˆé˜²æ­¢åƒ "Update" è¿™ç§é€šç”¨æ ‡é¢˜è¯¯æ€ï¼‰ï¼Œä½† filter_keywords åº”è¯¥å·²ç»è¦†ç›–äº†ä¸€äº›
        if len(norm_title) > 5: 
            if norm_title in seen_titles:
                logger.info(f"  ğŸš« æ ‡é¢˜é‡å¤ (Redundancy)ï¼Œè·³è¿‡")
                continue
            seen_titles.add(norm_title)

        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ content (ä¾‹å¦‚æ¥è‡ªæµ‹è¯•æ•°æ®æˆ– RSS å…¨æ–‡)
        content = article.get('content', '')
        summary = article.get('summary', '')
        
        if content and len(content) > 200:
             logger.info(f"  âœ“ ä½¿ç”¨å·²æœ‰æ­£æ–‡ ({len(content)} å­—ç¬¦)")
        elif summary and len(summary) > 500:
            logger.info(f"  âœ“ æ‘˜è¦è¾ƒé•¿ ({len(summary)} å­—ç¬¦)ï¼Œè·³è¿‡ç½‘é¡µæŠ“å–")
            content = summary
        else:
            logger.info(f"  â†’ å¼€å§‹æŠ“å–ç½‘é¡µå†…å®¹...")
            fetched_content = fetch_article_content(article['link'])
            if fetched_content:
                content = fetched_content
            logger.info(f"  âœ“ æŠ“å–å®Œæˆ: {len(content)} å­—ç¬¦")
            
        # 2. é•¿åº¦è¿‡æ»¤ (Pre-filtering)
        min_length = PROJ_CONFIG.get("filter_min_length", 100)
        if len(content) < min_length:
            logger.info(f"  ğŸš« å†…å®¹å¤ªçŸ­ ({len(content)} < {min_length})ï¼Œè·³è¿‡")
            continue
        
        analysis = analyze_article_with_llm(article['title'], summary, content)
        
        # æ ¼å¼åŒ–è¾“å‡º Verdict
        verdict = analysis.get('verdict', 'æœªçŸ¥')
        score = analysis['score']
        # æ·»åŠ  Red Flag æ ‡è¯†
        if 'red_flags' in analysis.get('detailed_scores', {}) and analysis['detailed_scores']['red_flags']:
             red_flags = analysis['detailed_scores']['red_flags']
             logger.info(f"  âš ï¸ å‘ç° Red Flags: {red_flags}")
             verdict = f"ğŸš« {verdict}"
            
        logger.info(f"  âœ“ è¯„åˆ†: {score:.1f}/5.0 - {verdict}")
        logger.info(f"  âœ“ è¯„ä»·: {analysis.get('reason', '')}")
        if 'detailed_scores' in analysis:
            scores = analysis['detailed_scores']
            logger.info(f"     ç›¸å…³æ€§:{scores['relevance']} ä¿¡æ¯é‡:{scores['informativeness']} "
                       f"æ·±åº¦:{scores['depth']} å¯è¯»æ€§:{scores['readability']} åŸåˆ›æ€§:{scores['originality']}")

        
        analyzed_articles.append({**article, "analysis": analysis})

        if article.get('id'):
            processed_ids.append(article['id'])
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ŒæŒ‰æœˆä»½ç»„ç»‡
    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜åˆ†æç»“æœåˆ°å½’æ¡£ç›®å½•
    analyzed_file = os.path.join(output_dir, f"analyzed_articles_{timestamp}.json")
    save_articles(analyzed_articles, analyzed_file)
    
    # åŒæ—¶ä¿å­˜åˆ°æ ¹ç›®å½•ï¼ˆä¸ºäº†å…¼å®¹æ€§å’Œæ–¹ä¾¿è®¿é—®ï¼‰
    save_articles(analyzed_articles, 'analyzed_articles.json')
    
    logger.info(f"\nåˆ†æç»“æœå·²ä¿å­˜åˆ°:")
    logger.info(f"  - {analyzed_file}")
    logger.info(f"  - analyzed_articles.json (æœ€æ–°ç‰ˆæœ¬)")
    
    # æ ‡è®°å·²è¯»
    if args.mark_read and processed_ids:
        logger.info(f"\næ­£åœ¨æ ‡è®° {len(processed_ids)} ç¯‡æ–‡ç« ä¸ºå·²è¯»...")
        feedly_mark_read(processed_ids)
    
    # ç”Ÿæˆæ€»ä½“æ‘˜è¦
    logger.info("\nç”Ÿæˆæ€»ä½“æ‘˜è¦...")
    overall_summary = generate_overall_summary(analyzed_articles)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ŒæŒ‰æœˆä»½ç»„ç»‡
    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    summary_file = os.path.join(output_dir, f"summary_{timestamp}.md")
    
    # åŒæ—¶ä¿å­˜åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆåœ¨æ ¹ output ç›®å½•ï¼‰
    latest_file = os.path.join("output", "summary_latest.md")
    
    # ä¿å­˜æ‘˜è¦ï¼ˆä¸ analyzed_articles åœ¨åŒä¸€ç›®å½•ï¼‰
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    with open(latest_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    
    logger.info(f"æ€»ä½“æ‘˜è¦å·²ä¿å­˜åˆ°:")
    logger.info(f"  - {summary_file}")
    logger.info(f"  - {latest_file}")
    logger.info(f"\nå½’æ¡£æ–‡ä»¶:")
    logger.info(f"  - {analyzed_file}")
    logger.info(f"  - {summary_file}")
    
    logger.info("\n" + "="*50)
    logger.info("æ€»ä½“æ‘˜è¦:")
    logger.info("="*50)
    logger.info(f"\n{overall_summary}")


if __name__ == "__main__":
    main()
