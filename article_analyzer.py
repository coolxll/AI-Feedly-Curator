#!/usr/bin/env python3
"""
AI-Feedly-Curator
ä½¿ç”¨ AI åˆ†æ RSS è®¢é˜…æ–‡ç« ï¼Œç”Ÿæˆè¯„åˆ†å’Œæ‘˜è¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python article_analyzer.py [--refresh] [--limit N] [--mark-read] [--debug]

é…ç½®ï¼š
    ä¿®æ”¹ src/config.py ä¸­çš„ PROJ_CONFIG æ¥è°ƒæ•´é»˜è®¤è®¾ç½®
    ä¿®æ”¹ .env æ–‡ä»¶æ¥é…ç½® API å¯†é’¥å’Œ Profile
"""
import os
import argparse
import logging
import concurrent.futures

# å¯¼å…¥æ¨¡å—
from rss_analyzer.config import PROJ_CONFIG, setup_logging
from rss_analyzer.feedly_client import feedly_fetch_unread, feedly_mark_read
from rss_analyzer.article_fetcher import fetch_article_content
from rss_analyzer.llm_analyzer import analyze_article_with_llm, analyze_articles_with_llm_batch, generate_overall_summary
from rss_analyzer.utils import load_articles, save_articles, is_newsflash

logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Article Analyzer")
    
    parser.add_argument("--input", default=PROJ_CONFIG["input_file"], 
                        help=f"Input JSON file (default: {PROJ_CONFIG['input_file']})")
    parser.add_argument("--limit", type=int, default=PROJ_CONFIG["limit"], 
                        help=f"Number of articles to process (default: {PROJ_CONFIG['limit']})")
    parser.add_argument("--mark-read", action="store_true", default=PROJ_CONFIG["mark_read"], 
                        help=f"Mark processed articles as read (default: {PROJ_CONFIG['mark_read']})")
    parser.add_argument("--debug", action="store_true", default=PROJ_CONFIG["debug"], 
                        help=f"Enable debug mode (default: {PROJ_CONFIG['debug']})")
    parser.add_argument("--refresh", action="store_true", default=PROJ_CONFIG["refresh"],
                        help=f"Refresh from Feedly before processing (default: {PROJ_CONFIG['refresh']})")
    parser.add_argument("--stream-id", help="Feedly Stream ID to fetch from (Category/Feed)")
    parser.add_argument("--export", help="Export fetched articles to JSON file without analysis")
    parser.add_argument("--threads", type=int, help="Number of threads for concurrent batch scoring")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    debug_mode = args.debug or os.getenv("DEBUG", "").lower() in ("true", "1", "yes")
    setup_logging(debug_mode)

    if debug_mode:
        logger.info("Debugæ¨¡å¼å·²å¯ç”¨")

    # å¯¼å‡ºæ¨¡å¼ (Export Mode)
    if args.export:
        logger.info("=" * 60)
        logger.info(f"ğŸ“¤ å¯¼å‡ºæ¨¡å¼: å°†æŠ“å–æ–‡ç« å¯¼å‡ºåˆ° {args.export}")
        if args.stream_id:
            logger.info(f"Target Stream: {args.stream_id}")
        logger.info("=" * 60)

        logger.info(f"æ­£åœ¨è·å–æœ€æ–° {args.limit} ç¯‡æœªè¯»æ–‡ç« ...")
        articles = feedly_fetch_unread(limit=args.limit, stream_id=args.stream_id)

        if articles is None:
             logger.error("âŒ æ— æ³•ä» Feedly è·å–æ–‡ç« ï¼Œé€€å‡º")
             return

        save_articles(articles, args.export)
        logger.info(f"âœ“ æˆåŠŸå¯¼å‡º {len(articles)} ç¯‡æ–‡ç« åˆ° {args.export}")
        return

    # åˆ·æ–°unread_news.json
    if args.refresh:
        logger.info("=" * 60)
        logger.info("ğŸ“¥ ä» Feedly åˆ·æ–°æ–‡ç« ")
        if args.stream_id:
            logger.info(f"Target Stream: {args.stream_id}")
        logger.info("=" * 60)
        logger.info(f"æ­£åœ¨è·å–æœ€æ–° {args.limit} ç¯‡æœªè¯»æ–‡ç« ...")
        articles = feedly_fetch_unread(limit=args.limit, stream_id=args.stream_id)
        if articles is None:
            logger.error("âŒ æ— æ³•ä» Feedly è·å–æ–‡ç« ï¼Œé€€å‡º")
            return
        
        output_file = "unread_news.json"
        save_articles(articles, output_file)
        logger.info(f"âœ“ å·²ä¿å­˜ {len(articles)} ç¯‡æœªè¯»æ–‡ç« åˆ° {output_file}")
        logger.info("")
        
        if args.input == PROJ_CONFIG["input_file"]:
            args.input = output_file
    else:
        logger.info("=" * 60)
        logger.info("ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ–‡ç« æ•°æ®ï¼ˆæœªåˆ·æ–°ï¼‰")
        logger.info("=" * 60)
        logger.info("æç¤º: ä½¿ç”¨ --refresh å‚æ•°å¯ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        logger.info("")

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_file = args.input
    if not os.path.exists(input_file):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info("æç¤º: ä½¿ç”¨ --refresh å‚æ•°ä» Feedly è·å–æœ€æ–°æ–‡ç« ")
        return

    # åŠ è½½æ–‡ç« åˆ—è¡¨
    articles = load_articles(input_file)
    logger.info(f"ğŸ“– ä» {input_file} åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
    logger.info(f"ğŸ¯ å°†å¤„ç†å‰ {min(args.limit, len(articles))} ç¯‡æ–‡ç« ")
    logger.info("")
    
    analyzed_articles = []
    processed_ids = []
    seen_titles = set()
    batch_scoring = PROJ_CONFIG.get("batch_scoring", False)
    batch_size = max(1, int(PROJ_CONFIG.get("batch_size", 1)))
    batch_queue = []

    # å¹¶å‘å¤„ç†ç›¸å…³
    max_workers = args.threads or int(PROJ_CONFIG.get("max_workers", 3))
    executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
    pending_futures = []  # List of (future, batch_items)

    def process_completed_futures():
        """æ£€æŸ¥å¹¶å¤„ç†å·²å®Œæˆçš„ Future"""
        nonlocal pending_futures
        still_pending = []
        for future, batch_items in pending_futures:
            if future.done():
                try:
                    batch_results = future.result()
                    for item, analysis in zip(batch_items, batch_results):
                        record_analysis_result(item['article'], analysis)
                except Exception as e:
                    logger.error(f"Batch processing failed: {e}")
            else:
                still_pending.append((future, batch_items))
        pending_futures = still_pending

    def record_analysis_result(article_item, analysis_result):
        """å°†è¯„åˆ†ç»“æœæ ‡å‡†åŒ–è®°å½•åˆ°è¾“å‡ºåˆ—è¡¨"""
        verdict = analysis_result.get('verdict', 'æœªçŸ¥')
        score = analysis_result['score']
        if 'red_flags' in analysis_result.get('detailed_scores', {}) and analysis_result['detailed_scores']['red_flags']:
            red_flags = analysis_result['detailed_scores']['red_flags']
            logger.info(f"  âš ï¸ å‘ç° Red Flags: {red_flags}")
            verdict = f"ğŸš« {verdict}"

        title_str = article_item.get('title', 'Unknown Title')
        logger.info(f"  âœ…æ ‡é¢˜: {title_str}")
        logger.info(f"  âœ…è¯„åˆ†: {score:.1f}/5.0 - {verdict}")
        logger.info(f"  âœ…è¯„ä»·: {analysis_result.get('reason', '')}")
        if 'detailed_scores' in analysis_result:
            scores = analysis_result['detailed_scores']
            logger.info(f"     ç›¸å…³æ€§:{scores['relevance']} ä¿¡æ¯é‡:{scores['informativeness']} "
                        f"æ·±åº¦:{scores['depth']} å¯è¯»æ€§:{scores['readability']} åŸåˆ›æ€§:{scores['originality']}")

        analyzed_articles.append({**article_item, "analysis": analysis_result})
        if article_item.get('id'):
            processed_ids.append(article_item['id'])
    
    # æ”¶é›†æ‰€æœ‰å¾…å¤„ç†æ–‡ç« çš„ IDï¼ˆç”¨äºæ ‡è®°å·²è¯»ï¼ŒåŒ…æ‹¬è·³è¿‡çš„ï¼‰
    all_article_ids = [a['id'] for a in articles[:args.limit] if a.get('id')]
    
    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    try:
        for idx, article in enumerate(articles[:args.limit], 1):
            logger.info(f"å¤„ç†ç¬¬ {idx}/{min(args.limit, len(articles))} ç¯‡: {article['title']}")

            # 1. å…³é”®è¯è¿‡æ»¤ (Pre-filtering)
            filter_keywords = PROJ_CONFIG.get("filter_keywords", [])
            if any(kw in article['title'] for kw in filter_keywords):
                logger.info("  ğŸš« æ ‡é¢˜åŒ…å«è¿‡æ»¤è¯ï¼Œè·³è¿‡")
                continue

            # 1.2 URLæ¨¡å¼è¿‡æ»¤ (Pre-filtering)
            filter_url_patterns = PROJ_CONFIG.get("filter_url_patterns", [])
            article_url = article.get('link', '') or article.get('originId', '')
            if any(pattern in article_url for pattern in filter_url_patterns):
                logger.info(f"  ğŸš« URLåŒ¹é…è¿‡æ»¤è§„åˆ™ ({article_url})ï¼Œè·³è¿‡")
                continue

            # 1.3 ç®€å•å»é‡ (Redundancy Filter)
            norm_title = "".join(filter(str.isalnum, article['title'].lower()))
            # æ£€æŸ¥æ˜¯å¦å¤ªçŸ­ï¼ˆé˜²æ­¢åƒ "Update" è¿™ç§é€šç”¨æ ‡é¢˜è¯¯æ€ï¼‰ï¼Œä½† filter_keywords åº”è¯¥å·²ç»è¦†ç›–äº†ä¸€äº›
            if len(norm_title) > 5:
                if norm_title in seen_titles:
                    logger.info("  ğŸš« æ ‡é¢˜é‡å¤ (Redundancy)ï¼Œè·³è¿‡")
                    continue
                seen_titles.add(norm_title)

            # 1.4 å¿«è®¯è¿‡æ»¤ (Newsflash Filter)
            if is_newsflash(article):
                logger.info("  ğŸš« è¯†åˆ«ä¸ºå¿«è®¯ (Newsflash)ï¼Œè·³è¿‡")
                continue

            # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ content (ä¾‹å¦‚æ¥è‡ªæµ‹è¯•æ•°æ®æˆ– RSS å…¨æ–‡)
            content = article.get('content', '')
            summary = article.get('summary', '')

            if content and len(content) > 200:
                 logger.info(f"  âœ“ ä½¿ç”¨å·²æœ‰æ­£æ–‡ ({len(content)} å­—ç¬¦)")
            elif summary and len(summary) > 500:
                logger.info(f"  âœ“ æ‘˜è¦è¾ƒé•¿ ({len(summary)} å­—ç¬¦)ï¼Œè·³è¿‡ç½‘é¡µæŠ“å–")
                content = summary
            else:
                logger.info("  â†’ å¼€å§‹æŠ“å–ç½‘é¡µå†…å®¹...")
                fetched_content = fetch_article_content(article['link'])
                if fetched_content:
                    content = fetched_content
                logger.info(f"  âœ“ æŠ“å–å®Œæˆ: {len(content)} å­—ç¬¦")

            # 2. é•¿åº¦è¿‡æ»¤ (Pre-filtering)
            min_length = PROJ_CONFIG.get("filter_min_length", 100)
            if len(content) < min_length:
                logger.info(f"  ğŸš« å†…å®¹å¤ªçŸ­ ({len(content)} < {min_length})ï¼Œè·³è¿‡")
                continue

            if batch_scoring:
                batch_queue.append({
                    'article': article,
                    'title': article.get('title', ''),
                    'summary': summary,
                    'content': content
                })
                if len(batch_queue) >= batch_size:
                    batch_payload = [
                        {
                            'title': item['title'],
                            'summary': item['summary'],
                            'content': item['content']
                        } for item in batch_queue
                    ]
                    # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                    logger.info(f"  >>> æäº¤æ‰¹é‡è¯„åˆ†ä»»åŠ¡ (Batch Size: {len(batch_payload)})")
                    future = executor.submit(analyze_articles_with_llm_batch, batch_payload)
                    pending_futures.append((future, list(batch_queue)))
                    batch_queue = []

                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæˆçš„ä»»åŠ¡
                process_completed_futures()

            else:
                analysis = analyze_article_with_llm(article['title'], summary, content)
                record_analysis_result(article, analysis)

        # å¤„ç†å‰©ä½™çš„é˜Ÿåˆ—
        if batch_scoring and batch_queue:
            batch_payload = [
                {
                    'title': item['title'],
                    'summary': item['summary'],
                    'content': item['content']
                } for item in batch_queue
            ]
            logger.info(f"  >>> æäº¤æœ€åæ‰¹é‡è¯„åˆ†ä»»åŠ¡ (Batch Size: {len(batch_payload)})")
            future = executor.submit(analyze_articles_with_llm_batch, batch_payload)
            pending_futures.append((future, list(batch_queue)))
            batch_queue = []

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if batch_scoring:
            logger.info("ç­‰å¾…æ‰€æœ‰è¯„åˆ†ä»»åŠ¡å®Œæˆ...")
            # é˜»å¡ç­‰å¾…å‰©ä½™çš„ä»»åŠ¡
            for future, batch_items in pending_futures:
                try:
                    batch_results = future.result()
                    for item, analysis in zip(batch_items, batch_results):
                        record_analysis_result(item['article'], analysis)
                except Exception as e:
                    logger.error(f"Batch processing failed: {e}")

    finally:
        executor.shutdown(wait=True)



    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜åˆ†æç»“æœåˆ°å½’æ¡£ç›®å½•
    analyzed_file = os.path.join(output_dir, f"analyzed_articles_{timestamp}.json")
    save_articles(analyzed_articles, analyzed_file)
    
    # åŒæ—¶ä¿å­˜åˆ°æ ¹ç›®å½•ï¼ˆä¸ºäº†å…¼å®¹æ€§å’Œæ–¹ä¾¿è®¿é—®ï¼‰
    save_articles(analyzed_articles, 'analyzed_articles.json')
    
    logger.info("\nåˆ†æç»“æœå·²ä¿å­˜åˆ°:")
    logger.info(f"  - {analyzed_file}")
    logger.info("  - analyzed_articles.json (æœ€æ–°ç‰ˆæœ¬)")
    
    # æ ‡è®°å·²è¯»ï¼ˆæ‰€æœ‰æŠ“å–çš„æ–‡ç« ï¼ŒåŒ…æ‹¬è¢«è¿‡æ»¤/è·³è¿‡çš„ï¼‰
    if args.mark_read and all_article_ids:
        logger.info(f"\næ­£åœ¨æ ‡è®° {len(all_article_ids)} ç¯‡æ–‡ç« ä¸ºå·²è¯»...")
        feedly_mark_read(all_article_ids)
    
    # ç”Ÿæˆæ€»ä½“æ‘˜è¦
    logger.info("\nç”Ÿæˆæ€»ä½“æ‘˜è¦...")
    overall_summary = generate_overall_summary(analyzed_articles)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ŒæŒ‰æœˆä»½ç»„ç»‡
    from datetime import datetime
    now = datetime.now()
    month_dir = now.strftime("%Y-%m")  # ä¾‹å¦‚: 2026-01
    output_dir = os.path.join("output", month_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    summary_file = os.path.join(output_dir, f"summary_{timestamp}.md")
    
    # åŒæ—¶ä¿å­˜åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆåœ¨æ ¹ output ç›®å½•ï¼‰
    latest_file = os.path.join("output", "summary_latest.md")
    
    # ä¿å­˜æ‘˜è¦ï¼ˆä¸ analyzed_articles åœ¨åŒä¸€ç›®å½•ï¼‰
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    with open(latest_file, 'w', encoding='utf-8') as f:
        f.write(overall_summary)
    
    logger.info("æ€»ä½“æ‘˜è¦å·²ä¿å­˜åˆ°:")
    logger.info(f"  - {summary_file}")
    logger.info(f"  - {latest_file}")
    logger.info("\nå½’æ¡£æ–‡ä»¶:")
    logger.info(f"  - {analyzed_file}")
    logger.info(f"  - {summary_file}")
    
    logger.info("\n" + "="*50)
    logger.info("æ€»ä½“æ‘˜è¦:")
    logger.info("="*50)
    logger.info(f"\n{overall_summary}")


if __name__ == "__main__":
    main()
